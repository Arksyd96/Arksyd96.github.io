[
    {
        "title": "Bases en probabilités pour du Machine learning (In French)",
        "description": "Mes notes sur les différents sujets de probabilités utils pour faire du Machine learning (Variables aléatoire,  Fonctions de distribution, Chaîne de Markov)",
        "tags": ["Probabilities", "Machine learning", "Mathematics"],
        "date": "October 22, 2021",
        "id": "mes_notes_en_probabilites",
        "body": {
            "cells": [
                {
                    "cell_type": "markdown",
                    "metadata": {},
                    "source": [
                        "(Référence des formules: Bibm@th)\n",
                        "# **Les variables aléatoires** \n",
                        "## **1. Variables aléatoires discrètes :**\n",
                        "Une variable aléatoire discrète est une application $X$ de $\\Omega$ (Univers, contenant toutes les possibilités) dans $E$ tel que $X(\\Omega)$ soit fini ou dénombrable.\n",
                        "- Notons : $X(\\Omega) = \\{x_n; n \\in I\\}$ où $I$ est fini ou dénombrable.\n",
                        "- La loi de probabilité de $X$, soit $(P_n)_{n \\; \\in \\; I}$, est noté : $P_n =P(X=x_n)$.\n",
                        "\n",
                        "**Note**: Un ensemble dénombrable est un ensemble en bijection avec l'ensemble des entiers $\\mathbb{N}$ (équipotent).\n",
                        "\n",
                        "- La probabilité $P(y|x)$ est la probabilité conditionnelle de $y$ sachant $x$ (Probabilité d'un évenement sachant qu'un autre évenement a eu lieu), et est définit comme suit :\n",
                        "$$ P(Y = y|X = x) = \\frac{P(X = x, Y = y)}{P(X = x)}$$\n",
                        "où $P(X = x, Y = y)$ est l'application de la **loi conjointe** sur les variables $X$ et $Y$. \n",
                        "- La loi conjointe entre deux variables $X$ et $Y$ est la loi qui définit toutes les probabilités du vecteur $(X, Y)$ dans un ensemble $\\mathbb{E}^2$, en d'autres termes, la probabilité de l'apparition de $X$ et $Y$ en même temps $P((X = x) \\cap (Y = y))$ (à ne pas confondre avec la probabilité conditionnelle) :\n",
                        "$$ P_{X,Y}(I \\times J) = P(X \\in I \\; \\; et \\; \\; Y \\in J) $$  \n",
                        "***Exemple***: Soit $E$ un évenement où on tire 2 valeurs dans l'univers $\\{-1, 1\\}^2$. $X$ est la somme des deux valeurs et $Y$ leur produit. On aura alors $X \\in \\{-2, 0, 2\\}$ et $Y \\in \\{-1, 1\\}$. Voici les valeurs possible de la loi conjointe entre $X$ et $Y$ :\n",
                        "$$ P(X = -2, Y = -1) = 0, \\;\\;\\;\\; P(X = 0, Y = -1) = \\frac{1}{2}$$\n",
                        "$$ P(X = 2, Y = -1) = 0, \\;\\;\\;\\; P(X = -2, Y = 1) = \\frac{1}{4}$$\n",
                        "$$ P(X = 0, Y = 1) = 0, \\;\\;\\;\\; P(X = 2, Y = 1) = \\frac{1}{4}$$  \n",
                        "**Note**: faites attention à l'ordre des variables dans la loi conjointe.  \n",
                        "**Note**: Si $P(X = x, Y = y)$ est appellé loi conjointe, alors $P(X = x), P(Y = y)$ sont appellées **lois marginales**.  \n",
                        "- Il possible de retrouver les lois marginales à partir de la loi conjointe en fixant la valeur d'une variable $X \\in \\{x_1, ..., x_p\\}$ ou $Y \\in \\{y_1, ..., y_q\\}$:\n",
                        "$$ P(X = x_i) = \\sum_{j=1}^q P(X = x_i, Y = y_j) \\;\\; et \\;\\; P(Y = y_j) = \\sum_{i=1}^p P(X = x_i, Y = y_j)$$ \n",
                        "- Cependant l'inverse n'est pas toujours correcte, il est possible de trouver la loi conjointe entre deux variables seulement si ces dérnières sont indépendentes, et dans ce cas là on aura :\n",
                        "$$ P(X = x, Y = y) = P(X=x)P(Y=y)$$  \n",
                        "Si les deux variables ne sont pas indépentes alors il faudra plus d'informations sur le rapport entre $X$ et $Y$ :\n",
                        "$$ P(X = x, Y= y) = P(X = x)P(Y = y | X = x) \\;\\; (origine \\;\\; de \\;\\; la \\;\\; loi \\;\\; conditionnelle)$$\n",
                        "\n",
                        "**I. Espérence d'une variable aléatoire discrète:** notée $\\mathbb{E}[X]$, est la valeur moyenne qu'on s'attend à trouver pour la variable aléatoire $X$. La formule de l'espérence est définit comme suit :\n",
                        "$$ \\mathbb{E}(X) = \\sum_{n \\in I} x_n P(X = x_n)$$\n",
                        "**II. Variance d'une variable aléatoire discrète:** notée $V[X]$, la mesure de dispersion des valeurs de $X$. Elle est définit comme suit :\n",
                        "$$ V[X] = \\mathbb{E}[(X - \\mathbb{E}[X])^2] = \\mathbb{E}[X^2] - \\mathbb{E}[X]^2$$\n",
                        "- *Démonstration* de $V[X] \\Longrightarrow$ :  \n",
                        "$= \\mathbb{E}[(X - \\mathbb{E}[X])^2] = \\mathbb{E}[X^2 - 2X\\mathbb{E}[X] + \\mathbb{E}[X]^2]$  \n",
                        "$= \\mathbb{E}[X^2] - 2\\mathbb{E}[X]\\mathbb{E}[X] + \\mathbb{E}[X]^2 = \\mathbb{E}[X^2] - 2\\mathbb{E}[X]^2 + \\mathbb{E}[X]^2$  \n",
                        "$= \\mathbb{E}[X^2] - \\mathbb{E}[X]^2$   \n",
                        "**Note** $\\mathbb{E}[\\mathbb{E}[X]] = \\mathbb{E}[X]$  \n",
                        "\n",
                        "## **2. Variables aléatoires continue (Variables à densité):**\n",
                        "Une variable aléatoire continue est une fonction $X$ de $\\Omega$ dans $\\mathbb{R}$ $(X:\\Omega \\rightarrow \\mathbb{R})$ telle qu'il existe $f: \\mathbb{R} \\rightarrow \\mathbb{R}$ continue par morceaux et vérifiant :\n",
                        "$$ \\forall a < b : P(X \\in [a, b]) = \\int_a^b f(x)dx$$\n",
                        "- Si une telle fonction existe, elle est alors appellé densité de la variable aléatoire $X$. Cette fonction doit nécéssairement être positive et vérifie $\\int_{- \\infty}^{+ \\infty} f(x)dx = 1$ (l'air de la fonction $f$ correspond à une probabilité, et la somme des probabilités de tout les évenements $x_n$ dans $\\mathbb{R}$ doit être égale à 1).\n",
                        "- On pose alors la loi suivante :\n",
                        "$$ f(x) = \\frac{dF(x)}{dx} $$\n",
                        "où $F(x)$ est la fonction de répartition de la variable. \n",
                        "- Toutes variables continue est définit par une certaine fonction de répartition, et en intégrant cette dérinère on obtient la fonction de densité.\n",
                        "\n",
                        "**1. Espérence d'une variable aléatoire continue**: de la même manière que pour les variables discrètes, sauf qu'on utilise une intégrale étant donné la continuité de la fonction :\n",
                        "$$ \\mathbb{E}[X] = \\int_{-\\infty}^{+\\infty} xf(x)dx $$\n",
                        "**2. Variance d'une variable aléatoire continue** même formule : $ V[X] = \\mathbb{E}[X^2] - \\mathbb{E}[X]^2$\n",
                        "\n",
                        "## **3. Fonctions de répartition les plus connues :**\n",
                        "### **1. Loi géométrique :**\n",
                        "Une variable aléatoire discrète $X$ suit une loi géométrique de paramètres $p$, qu'on note $X \\sim \\mathcal{G}(p)$ si :\n",
                        "- $X(\\Omega) = \\mathbb{N}^{*}$\n",
                        "- $P(X = k) = p(1 - p)^{k - 1} = pq^{k - 1}$  \n",
                        "\n",
                        "$X$ admet alors une espérence et une variance :\n",
                        "- $\\mathbb{E}[X] = \\frac{1}{p}$  \n",
                        "- $V[X] = \\frac{q}{p^2} = \\frac{1 - p}{p^2}$ \n",
                        "\n",
                        "**Exemple**: Lancer de pièce de monnaie truquée avec les probabilités $P(pile) = p$ et $P(face) = 1 - p = q$. soit $X$ le nombre de lancers pour obtenir pile pour la première fois, $X$ suit alors une loi géométrique. Dans cet exemple, le paramètre $k$ est le nombre de lancers total.  \n",
                        "Pour $k = 5$ (5 lancers), la probabilité d'avoir pile pour la première fois est $pqqqq = pq^4$. L'espérence dans ce cas et la probabilité moyenne d'avoir pile, avec $k = 5$ on obtient $\\mathbb{E}[X = 5] = \\frac{1}{5} = 20\\%$ de chances.\n",
                        "\n",
                        "### **2. Loi de Poisson :**\n",
                        "Soit $\\lambda > 0$. Une variable aléatoir discrète $X$ suit une loi de Poisson de paramètre $\\lambda$ et noté $X \\sim \\mathcal{P}(\\lambda)$ si :\n",
                        "- $X(\\Omega) = \\mathbb{N}$\n",
                        "- $P(X = k) = e^{-\\lambda}\\frac{\\lambda^{k}}{k!}$\n",
                        "\n",
                        "$X$ admet alors une espérence et une variance :\n",
                        "- $\\mathbb{E}[X] = \\lambda$\n",
                        "- $V[X] = \\lambda$\n",
                        "\n",
                        "*Note* la loi de Poisson est la loi des phénomènes rares et des petites probabilités, généralement utiliser pour modéliser des systèmes de file d'attente. Pour plus d'informations, consulter ce [lien](https://fr.wikipedia.org/wiki/Loi_de_Poisson).\n",
                        "\n",
                        "### **3.Loi exponentielle :**\n",
                        "$X$ est une variable aléatoire continue suivant la loi exponentielle de paramètre $a > 0$, note $X \\sim \\mathcal{e}(a)$ si elle est absolument continue et admet pour densité :\n",
                        "- $f(x) = x =\\begin{cases}ae^{-ax} & si \\;\\; x > 0\\\\0 & sinon\\end{cases}$  \n",
                        "\n",
                        "$X$ admet une espérence et une variance :\n",
                        "- $\\mathbb{E}[X] = \\frac{1}{a}$\n",
                        "- $V[X] = \\frac{1}{a^2}$\n",
                        "\n",
                        "*Note* la loi exponentielle est la version continue de la loi géométrique; Sert souvent à modéliser la durée de vie d'une entité. $X$ dans ce cas là est une variable continue sans mémoire, et elle vérifie donc :$\\forall x, y \\geq 0$ : $P(X \\geq x+y |X \\geq y) = P(X \\geq x)$ (car on traite généralement des périodes de temps avec la loi exponentielle).  \n",
                        "\n",
                        "La répartition de $X$ est : $F(t) = \\begin{cases}0 & si \\;\\; t \\leq 0\\\\ 1 - e^{-at} & sinon \\end{cases}$  \n",
                        "Je note bien $t$ et non pas $x$ car l'unité est le temps.\n",
                        "\n",
                        "### **4. Loi uniforme :**\n",
                        "$X$ est une variable aléatoire continue répartit de manière uniforme sur un ensemble $[a, b]$, noté $X \\sim \\mathcal{U}(a, b)$ si elle admet comme fonction de densité :\n",
                        "- $f(x) = \\begin{cases} \\frac{1}{b - a} & x \\in [a, b]\\\\0 & sinon \\end{cases}$  \n",
                        "\n",
                        "et la fonction de répartition de $X$ est alors :\n",
                        "- $F(x) = \\begin{cases} 0 & si \\;\\; t \\leq x \\\\ \\frac{x - a}{b - a} & si \\;\\; x \\in [a, b] \\\\ 1 & sinon\\end{cases}$\n",
                        "\n",
                        "On donne aussi : $\\mathbb{E}[X] = \\frac{a+b}{2}$ et $V[X] = \\frac{(b-a)^2}{12}$.\n",
                        "\n",
                        "### **5. Loi normale (ou Gaussienne) :** (Important pour le machine learning :D)\n",
                        "$X$ est une variable aléatoire continue dont la réparition est normale (ou Gaussienne) avec les paramètres $m$ (espérence) et $\\sigma^2$ (variance), et que l'on note $X \\sim \\mathcal{N}(m, \\sigma^2)$, si elle est continue et a pour densité :\n",
                        "- $f(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}}exp(-\\frac{(x - m)^2}{2\\sigma^2})$\n",
                        "\n",
                        "On aura alors $\\mathbb{E}(X) = m$ et $V[X] = \\sigma^2$\n",
                        "\n",
                        "# **Connaissance transversales en probabilités :**\n",
                        "## **1. Système d'événement complet :**\n",
                        "Soit $(\\Omega, A, P)$ un espace probabiliste. Un système complet d'événement est un systèmes où tout les événements réunis forment $\\Omega$. donc :\n",
                        "- $i \\neq j \\Longrightarrow A_i \\cup A_j = \\emptyset$;   (événemenents indépendants).\n",
                        "- $\\bigcup_{i \\in I} A_i = \\Omega$\n",
                        "\n",
                        "On parle aussi de système quasi-complet quand la deuxième condition est remplacée par $\\sum_{i \\in I} P(A_i) = 1$.\n",
                        "\n",
                        "## **2. Expérience ou schéma de Bernoulli :**\n",
                        "Une expérience de Bernoulli n'a que deux issues possibles, succès ($s$) ou échec ($\\overline{s}$).\n",
                        "Le succès est définit par une probabilité $p$ et l'échec par une probabilité $q = 1 - p$.\n",
                        "Le paramètre de l'expérience est $p$, la probabilité d'un succès.  \n",
                        "**Exemple**: Pour un lancer de dé, la probabilité d'avoir un 6, étant le succès, est définit par $p = \\frac{1}{6}$.  \n",
                        "$X$ est une variable aléatoire qui est définit par $X=1$ en cas de réussite et $X=0$ en cas d'échec. $X$ suit alors une loi de Bernoulli de paramètre $p$ note $X \\sim \\mathcal{B}(p)$.  \n",
                        "$X$ admet une espérence et une variance :\n",
                        "- $\\mathbb{E}[X] = 1 \\times p + 0 \\times (1 - p) = p$\n",
                        "- $V[X] = p(1-p)$\n",
                        "\n",
                        "## **3. Loi binomiale :**\n",
                        "On considère une expérience aléatoire qui ne possède que deux résultats : succès ($s$), échec ($\\overline{s}$). On pose :\n",
                        "- $p = P(s)$\n",
                        "- $q = P(\\overline{s}) = 1 - p$  \n",
                        "\n",
                        "On répète $n$ fois cette expérience, et on suppose que les répétitions sont indépendentes. On pose $X$ une variable aléatoire qui représente le nombre de succès au cours des $n$ répétitions, on dit alors que $X$ suit une loi binomiale de paramètre $n$ et $p$.  \n",
                        "La probabilité d'obtenir $k$ succès au cours de $n$ répétitions est noté :\n",
                        "$$P(X = k) = C^n_k \\times p^k(1 - p)^{n-k}$$  \n",
                        "\n",
                        "Où $C^n_k$, dit, $k$ parmis $n$, est le nombre de combinaisons de $k$ élements choisis parmi $n$.\n",
                        "$C$ est le coefficient binomial et est égale au nombre de chemins conduisant à strictement $k$ succès dans l'arbre représentant le schéma de Bernoulli. Plus formellement $C$ est définit comme suit :\n",
                        "$$C_k^n = \\frac{n!}{k!(n - k)!}$$\n",
                        "\n",
                        "*Note* Une expérience de Bernoulli est une loi Binomiale avec nombre de répétition $n = 1$. Une loi Binomiale est la répétition de $n$ fois le schéma de Bernoulli.\n",
                        "\n",
                        "#### Quelques propriété du coefficient Binomial\n",
                        "- $C_0^n = 1$, Nombre de chemin à 0 succès dans une suite de $n$ répétitions ? un seul chemin possible.\n",
                        "- De la même manière: $C_n^n = 1$.\n",
                        "- $C_1^n = n$, un seul succès sur $n$ répétitions: $n$ combinaisons possibles; $n=4 \\Longrightarrow (1000, 0100, 0010, 0001)$.\n",
                        "- Pour $0 \\leq k \\leq n$: $C^n_{n-k} = C^n_k$.\n",
                        "- Pour $0 \\leq k \\leq n$: $C^n_k + C^n_{k+1} = C^{n+1}_{k+1}$\n",
                        "- En utilisant les régles précédentes, on peut simplifier le calcul. **Exemple**\n",
                        "    - $C^4_2 = C^{3 + 1}_{1 + 1} = C^3_1 + C^3_2 = 3 + C^2_1 + C^2_2 = 3 + 2 +1 = 6$\n",
                        "\n",
                        "## **4. Loi de Bayes :**\n",
                        "Pour bien comprendre la loi de Bayes, il faut comprendre la notion de dépendance entre événements. Soit $A$ et $B$ deux événements :\n",
                        "- Si $A$ et $B$ sont indépendants, alors : \n",
                        "    - $P(A \\cap B) = P(A)P(B) = P(B)P(A) = P(B \\cap A)$  \n",
                        "    - $P(A | B) = \\frac{P(A \\cap B)}{P(B)} = \\frac{P(A)P(B)}{P(B)} = P(A)$\n",
                        "- Si $A$ et $B$ sont dépendants, alors : \n",
                        "    - $P(A \\cap B) = P(B)P(A|B) = P(A)P(B|A) = P(B \\cap A)$  \n",
                        "\n",
                        "$P(A \\cap B)$ est la probabilité de l'arrivée des événements $A$ et $B$. Dans le cas d'une dépendances entre ces dérniers, il faut alors prendre en considération l'ordre d'arrivé. Si $A$ est arrive en premier, la probabilité d'avoir $B$ lorsque $A$ est présent est définie par $P(B|A)$ ($B$ sachant $A$, et aussi noté $P_A(B)$). On aura donc $P(A \\cap B) = P(A) \\times P(B|A)$. Dans un deuxième cas où $B$ arrive en premier, on aura alors $P(B) \\times P(A|B)$.  \n",
                        "Par inférence, on a alors :\n",
                        "$$A \\cap B = B \\cap A \\Longleftrightarrow P(A \\cap B) = P(B \\cap A) \\Longleftrightarrow P(B)P(A|B) = P(A)P(B|A)$$  \n",
                        "Ce qui nous donne la loi de Bayes définit par :\n",
                        "$$ P(A|B) = \\frac{P(A)P(B|A)}{P(B)}$$  \n",
                        "Il est à noter aussi que dans un système d'événements complet $(\\Omega, A, P)$ où toutes les probabilités $P_i$ des événements $A_i$ sont non nulles. La loi de Bayes est définit par :\n",
                        "$$ B = B \\cap A_1 + B \\cap A_2 + ... + B \\cap A_n = \\sum_{i \\in A} B \\cap A_i$$\n",
                        "$$ P(B) = \\sum_{n \\geq 1} P(A_n) \\times P(B | A_n)$$  \n",
                        "Formule souvent utilisée quand le système est constitué de $A$ et $\\overline{A}$ :\n",
                        "$$ P(A|B) = \\frac{P(B|A)P(A)}{P(B|A)P(A) + P(B|\\overline{A})P(\\overline{A})} $$  \n",
                        "*Note* Attention, on se permet d'utiliser $\\overline{A}$ car on peut la déduire étant donné que le système n'est constitué que des deux événements contraires. Dans un systèmes avec plus d'événements, il faudra avoir plus d'informations sur les autres événements. De manière plus générale on notera :\n",
                        "$$ P(A_k|B) = \\frac{P(B|A_k)P(A_k)}{P(B)} = \\frac{P(B|A_k)P(A_k)}{\\sum_{i \\geq 1}^n P(A_i)P(B |A_i)} \\;\\; avec \\;\\; k \\neq i$$  \n",
                        "\n",
                        "**Exemple pour avoir une meilleure idée**: Posons deux événements $A$ = \"tomber malade\" et $B$ = \"être testé positif\". La probabilité de tomber malade ($A$ arrive en premier) puis être testé positif ($B$ arrive en second) est assez élévée, soit par exemple $90\\%$, si on suppose que le test est de bonne qualité. Cependant, la probabilité d'être testé positif ($B$ arrive en premier) puis de tomber malade ($A$ arrive en second) est assez basse, il est moins probable de tomber malade en ayant déjà été testé positif.  \n",
                        "Soit $P(A) = 30\\%$ et $P(B|A) = 90\\%$, en utilisant les formules précédentes, il est possible de calculer $P(A|B)$:\n",
                        "- $P(B) = P(A)P(B|A) + P(\\overline{A})P(B|\\overline{A}) = 0,3 \\times 0,9 + 0,7 \\times 0,1 = 0,34$\n",
                        "- $P(A|B) = \\frac{P(A)P(B|A)}{P(B)} = \\frac{0,3 \\times 0,9}{0,34} = 0,79$\n",
                        "\n",
                        "# **Processus stochastique**\n",
                        "Un processus stochastique est une fonction du temps dont la valeur à chaque instant $t$ dépend de l'issue d'une expérience aléatoire.\n",
                        "- Le temps peut être discret ou continu.\n",
                        "- L'ensemble $E$ est l'ensemble des états possibles que peut prendre la variable $X^{(t)}$.  \n",
                        "## Vecteur et matrice stochastique :\n",
                        "- Un vecteur $\\pi = (\\pi_0, \\pi_1, \\pi_2, ..., \\pi_n)$ est stochastique seulement si la somme des composantes est égale à 1 : $\\sum_i^n \\pi_i = 1$.\n",
                        "- Pareil pour une matrice, chaque ligne est un vecteur stochastique.  \n",
                        "**Exemple**\n",
                        "$$ \\pi = (\\frac{1}{4}, \\frac{1}{2}, \\frac{1}{4}); \\;\\;\\; P = \n",
                        "\\begin{bmatrix}\n",
                        "    \\frac{1}{2} & \\frac{1}{2} & \\frac{1}{4} & = 1\\\\ \n",
                        "    \\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} & = 1\\\\\n",
                        "    \\frac{1}{2} & 0 & \\frac{1}{2} & = 1\\\\ \n",
                        "\\end{bmatrix}$$\n",
                        "\n",
                        "## Processus sans mémoire (Processus Markovien) :\n",
                        "Un processus Markovien est un processus ayant la propriété de Markov. L'évolution du processus ne dépend que du présent, c-à-dire qu'à un instant $t+1$, on ne dépend que de l'état à l'instant $t$ :\n",
                        "$$ P[X_{n+1} = x_{n+1} | X_0=x_0; X_1=x_1; ...; X_n=x_n] = P[X_{n+1} = x_{x+1}|X_n=x_n]$$ \n",
                        "\n",
                        "# **Les chaînes de Markov**\n",
                        "## **1. Les chaînes de Markov à temps discret :**\n",
                        "$E$ est un espace d'états discrets, peut être fini ou dénombrable. ${X_n}_{n \\in E}$ est une variable aléatoire qui modélise une chaîne de Markov à temps discret (CMTD) seulement si :\n",
                        "$$ P[X_n = j | X_{n - 1} = i_{n - 1}; X_{n - 2} = i_{n-2}; ...; X_0 = i_0] = P[X_j = j|X_{n - 1} = i_{n-1}]$$  \n",
                        "$X$ représente un processus sans mémoire.  \n",
                        "### **Propriétés d'une CMTD :**\n",
                        "- On pose $P_{ij}$ la probabilité de transiter vers l'état $j$ sachant qu'à l'instant $t-1$ on etait à l'état $i$.\n",
                        "$$ P_{ij} = P[X_n = j | X_{n-1}=i]; \\;\\; \\forall n \\in \\mathbb{N}$$\n",
                        "- On note aussi la somme des probabilités d'une transition d'un état $i$ vers $j$ est égale à 1: $\\sum_{j \\in E} P_{ij} = 1$.\n",
                        "- Il est possible de transiter vers le même état : $P_{ii} \\geq 0$.  \n",
                        "**Exemple**  \n",
                        "$E = \\{1, 2, 3, 4\\}$; $P = [P_{ij}]; i,j \\in E$\n",
                        "$$ P = \\begin{bmatrix}\n",
                        "    0 & P_{12} & 0 & P_{14} & = 1 \\\\\n",
                        "    0 & 0 & 1 & 0 & =1 \\\\\n",
                        "    P_{31} & 0 & P_{33} & 0 & =1 \\\\\n",
                        "    1 & 0 & 0 & 0 & = 1 \n",
                        "\\end{bmatrix}$$  \n",
                        "*Note* Peut être représenté sous la forme d'un graphe pondéré où les poids sont les probabilités de transition.\n",
                        "\n",
                        "### **Distribution de l'état initial** :\n",
                        "- L'état initial est défini par le vecteur $\\pi^{(0)} = (\\pi^{(0)}_0, \\pi^{(0)}_1, \\pi^{(0)}_2, ..., \\pi^{(0)}_n)$ où $\\pi^{(0)}_i = P[X_0 = i]$ (Probabilité que la chaîne se trouve à l'état $i$ à l'instant $0$).  \n",
                        "- Si un système est initialement à l'état $j$ alors $\\pi^{(0)}_j = 1$ et $\\pi^{(0)}_i = 0$; $\\forall i \\neq j$.\n",
                        "\n",
                        "## **2. Analyse d'une chaîne de Markov à temps discret** :\n",
                        "### **Régime transitoire :**\n",
                        "Afin de déterminer le vecteur $\\pi^{(n)}$ des probabilités d'états à un instant $n$, on pose :\n",
                        "- État du vecteur $n$ : $\\pi^{(n)} = [\\pi^{(n)}_j]_{j \\in E} = [\\pi^{(n)}_1, \\pi^{(n)}_2, \\pi^{(n)}_3, ..., \\pi^{(n)}_j]$.\n",
                        "- Les valeurs de transition dépendent de la matrice de transition $P$ :\n",
                        "$$\\pi^{(n)}_j = P[X_n = j] = \\sum_{i \\in E} P[X_{n - 1} = i] \\times P[X_n = j | X_{n - 1} = i] \\;\\; Loi \\;\\; de \\;\\; Bayes$$\n",
                        "$$\\pi^{(n)}_j = \\sum_{i \\in E} \\pi^{(n-1)}_j \\times P_{ij}$$  \n",
                        "Par récursion jusqu'à $0$, on obtient :\n",
                        "$$\\pi^{(n)}_j = \\pi^{(n-1)} \\times P = \\pi^{(0)}P^n$$  \n",
                        "**Explication**  \n",
                        "$P[X_n = j]$ est la probabilité d'être à l'état $n$ à l'instant $j$ et est noté $\\pi{(n)}_j$. Pour obtenir cette probabilité d'être à l'état $n$, il faut sommer toutes les probabilités de transition vers cette état en démarrant de l'état précédent $i$ (Il faut considérer tout les chemins possibles). Plus formellement cela donne $P[X_{n - 1} = i] \\times P[X_n = j | X_{n - 1} = i]$ pour un seul chemin, en sommant le tout on obtient $\\Longleftrightarrow \\sum_{i \\in E} P[X_{n - 1} = i] \\times P[X_n = j | X_{n - 1} = i]$.  \n",
                        "Pour encore mieux comprendre, il faut savoir que $P[X_n = i]$ est simplement la probabilité d'être à l'état $i$ qu'on note $\\pi$. quant à $P[X_n = i| X_{n - 1} = j]$, c'est la probabilité de **transiter** de l'état $j$ vers $i$ et qu'on note $P$ (C'est une matrice).  \n",
                        "Quant à la récursion jusu'à $0$, voici un déroulement pour mieux comprendre :\n",
                        "- $\\pi^{(1)} = \\pi^{(0)} \\times P$\n",
                        "- $\\pi^{(2)} = \\pi^{(1)} \\times P = \\pi^{(0)} \\times P^2$\n",
                        "- $\\pi^{(3)} = \\pi^{(2)} \\times P = \\pi^{(0)} \\times P^3$\n",
                        "- ...\n",
                        "- $\\pi^{(n)} = \\pi^{(0)} \\times P^n$ *Appellé formule de probabilités totales*\n",
                        "\n",
                        "### **Évolution globale du processus $X_n$ ($m$ étapes) :**\n",
                        "Soit $P_{ij}^{(m)}$ la probabilité de transition de $i$ vers $j$ en $m$ étapes:\n",
                        "$$ P_{ij}^{(m)} = P[X_{n+m} = j|X_n = i] = \\sum_{k \\in E} P_{ik}^{(m - 1)} \\times P_{kj}$$  \n",
                        "Cette probabilité est égale à $\\pi^{(m)}_j$ (L'état $j$ est à l'étape $m$). il faut donc sommer toutes les probabilités de transition vers cette état en démarrant de l'état précédent $i$ (Il faut considérer tout les chemins possibles). Plus formellement cela donne $P[X_{n + m - 1} = i] \\times P[X_{n + m} = j | X_{n + m - 1} = i]$ pour un seul chemin, en sommant le tout on obtient $\\Longleftrightarrow \\sum_{i \\in E} P[X_{n + m - 1} = i] \\times P[X_{n + m} = j | X_{n + m - 1} = i]$. Pour encore mieux comprendre, il faut savoir que $P[X_n = i]$ est simplement la probabilité d'être à l'état $i$ qu'on note $\\pi$. Quant à $P[X_n = i| X_{n - 1} = j]$, c'est la probabilité de **transiter** de l'état $j$ vers $i$ et qu'on note $P$ (C'est une matrice).\n",
                        "\n",
                        "### **Temps de séjour :**\n",
                        "Le temps de séjoir en un état $i$ en $n$ étapes est le temps passé dans un état de la chaîne de Markov. Suivant une distribution géométrique (de paramètre $P_{jj}$ \\forall j \\in E), on peut définir la formule suivante :\n",
                        "$$ P[T=k] = p^k \\times (1-p)$$\n",
                        "Avec $T$ est le temps de séjour compté en nombre d'étapes et $p$ la probabilité de quitter l'état courant. Dans le cas d'une CMTD avec plusieurs noeuds, on peut définir $p$ comme étant la somme des probabilités de transitions à partir de l'état courant (On transforme la chaîne en une chaîne de deux états seulement, l'état courant et le reste).  \n",
                        "\n",
                        "La formule peut être retrouvé par récursion :\n",
                        "- $P[T = 1] = 1 - p$  \n",
                        "- $P[T = 2] = p \\times (1 - p)$  \n",
                        "- $P[T = 3] = p^{2} \\times (1 - p)$  \n",
                        "- $...$  \n",
                        "- $P[T = k] = p^{k - 1} \\times (1 - p)$  \n",
                        "\n",
                        "### **Temps de séjour moyenne:**\n",
                        "Le temps de séjour moyen d'un état $i$ est défini par l'espérance de la loi géométrique :\n",
                        "$$\\mathbb{E}[T] = \\frac{1}{p}$$\n",
                        "\n",
                        "### **CMTD irréductible :**\n",
                        "Une CMTD est dites irréductible si et seulement si on peut atteindre n'importe quel état $j$ à partir de $i$ avec $i \\neq j$ et en un nombre fini d'étapes (c-à-d qu'il n'y a pas de boucle dans la chaîne, sinon ça peut engendrer une faible probabilité de boucler infiniment).  \n",
                        "$$\\forall i, j \\in E, \\exists m > 1 \\Longrightarrow P_{ij}^{(m)} \\neq 0$$  \n",
                        "Une chaînes **non** irréductible possède au moins une sous chaîne absorbante.  \n",
                        "\n",
                        "## **3. Classification des états d'une chaîne de Markov:**\n",
                        "### 1. État périodique :\n",
                        "On peut définir un état $i$ comme périodique si et seulement si on peut revenir après un nombre d'étapes multiple de $k > 1$ (c-à-d qu'il y a une boucle dans la chaîne):\n",
                        "$$\\exists k> 1 \\;\\;tel \\;\\; que \\;\\; P_{ij}^{(m)} = 0 \\;\\; pour \\;\\; tout \\;\\; m \\in \\mathbb{N} \\;\\; et \\;\\; m \\% k \\neq 0$$\n",
                        "On dit alors que la période de l'état $i$ est le plus grand entier $k$ vérifiant la propriété. La période d'une CMTD est le PGCD des périodes de tout ses états. Si la période est égale à $1$, alors la CMTD est dite **apériodique**.  \n",
                        "### 2. État transitoire et récurrent :\n",
                        "Avant tout il faut définir la probabilité $f_{jj}$ de transition de l'état $j$ vers l'état $j$ (c-à-d $P_{jj}^{(m)} = 1$). \n",
                        "- On peut définir un état $i$ comme transitoire si et seulement si $f_{ij}$ est supérieur à $f_{ji}$ (c-à-d $P_{ij}^{(m)} > P_{ji}^{(m)}$, il est plus probable d'y sortir que d'y entrer. Il est donc transitoire). \n",
                        "- On peut définir un état $i$ comme récurrent si et seulement si $f_{ij}$ est inférieur à $f_{ji}$ (c-à-d $P_{ij}^{(m)} < P_{ji}^{(m)}$, de la même manière, il est plus probable d'y entrer que d'y sotir. Il est donc récurrent).  \n",
                        "\n",
                        "Pour faire beaucoup plus simple, on définit les formule suivante :\n",
                        "$$f_{ij} = P_{ij}^{(m)} = \\sum_{m \\in \\mathbb{N}}^\\infty P_{ij}^{(m)}$$  \n",
                        "Un état est :\n",
                        "- **Transitoire** si et seulement si $f_{jj} < 1$\n",
                        "- **Récurrent** si et seulement si $f_{jj} = 1$. De plus :\n",
                        "    - Il est **récurrent nul** si le temps moyen de retour est infini. Noté $M_j = \\infty$ (M_j est le temps moyen de retour vers $j$).\n",
                        "    - Il est **récurrent non nul** si le temps moyen de retour est fini. Noté $M_j = \\sum_{n=1}^{\\infty} nf_{jj}^{(n)}$ (Revoir la formule de l'espérence d'une variable discrète pour comprendre l'origine de la formule)."
                    ]
                },
                {
                    "cell_type": "markdown",
                    "metadata": {},
                    "source": [
                        "## **3. Ergodicité d'une chaîne de Markov:**\n",
                        "Une chaîne de Markov est dite **ergodique** si et seulement si elle est irréductible et que tous ses états sont transitoires (tout les états sont atteignables). Plus formellement, la chaîne est érgodique si $\\lim_{t \\rightarrow \\infty} \\pi^{(t)}$ existe est ne dépend pas du vecteur initial $\\pi^{(0)}$. En plus, une chaîne est :\n",
                        "- Irréductible si tout ses états sont de la même nature.\n",
                        "- Irréductible finie si tout ses états sont récurrents non nuls.\n",
                        "\n",
                        "### Le nombre moyen de passage par un état\n",
                        "Soit $R_{ij}$ le nombre moyen de passages par un état $j$ sachant que l'on vient d'un état $i \\neq j$ :\n",
                        "$$R_{ij} = \\sum_{n=1}^{\\infty} n \\times P[Y=n_{passages} |X_{t-1}=i]$$\n",
                        "Avec $Y$ le nombre de passages $n$ par un état $j$ et $X_{t-1}$ l'état précédent. On note aussi :\n",
                        "$$R_{ij} = \\frac{f_{ij}}{1 - f_{jj}}$$\n",
                        "c-à-dire que $R_{ij}$ est égal à la probabilité de transition d'un état $i$ vers l'état $j$ (un passage de $j$) divisé par la probabilité de transition de tout les autres états sauf vers lui même (tout les chemins possibles sauf les boucles internes de $j$ vers $j$ car ce ne sont pas réellement des passages mais plutot une station sur le même état).\n",
                        "\n",
                        "## **4. Régime permanent:**\n",
                        "Il s'agit de l'étude d'un état stationnaire de $\\pi$ si ce dérnier existe. On peut définir un régime permanent comme un état stationnaire de $\\pi$ si et seulement si :\n",
                        "$$\\lim_{t \\rightarrow \\infty} \\pi^{(t)} = \\pi^{(0)}P^n \\Longrightarrow \\pi^{(n)} = \\pi^{(n+1)}$$\n",
                        "il faut donc résoudre le système d'equations suivant :\n",
                        "$$\\pi = \\pi P$$\n",
                        "\n",
                        "Cependant, il existe des condtions nécéssaires mais pas suffisantes pour qu'un régime permanent soit vrai. La chaîne de Markov doit être irréductible (donc apériodique), et les états doivent être transitoires (érgodique). On aura alors $\\lim_{n \\rightarrow \\infty} \\pi^{(n)} = \\pi^{(p)}$ existe et ne dépend pas du vecteur initial $\\pi^{(0)}$. \n",
                        "$$ \\pi_j = \\sum_{i \\in E} \\pi_{i} P_{ij} = \\sum_{j} P_{ji} = \\pi P$$\n",
                        "Car les flux sortants sont égaux aux flux entrants, et de ce fait $\\sum_{i \\in E} P_{ij} = 1$.  \n",
                        "**Rappel** la formule suivante est très utile pour résoudre les systèmes d'équations :\n",
                        "$$\\sum_{i \\in E} \\pi_i = 1$$  \n",
                        "\n",
                        "Si la probabilité stationnaire existe, alors il existe un régime permanent et on note :\n",
                        "$$\\pi_i = \\frac{1}{M_i}$$"
                    ]
                }
            ],
            "metadata": {
                "interpreter": {
                    "hash": "101b61497117ea3f18d4e0f8cf93eb2d64c16663f47aa00fa1289b89b66d7e41"
                },
                "kernelspec": {
                    "display_name": "Python 3.6.12 64-bit ('deeplearning': conda)",
                    "name": "python3"
                },
                "language_info": {
                    "codemirror_mode": {
                        "name": "ipython",
                        "version": 3
                    },
                    "file_extension": ".py",
                    "mimetype": "text/x-python",
                    "name": "python",
                    "nbconvert_exporter": "python",
                    "pygments_lexer": "ipython3",
                    "version": "3.6.12"
                },
                "orig_nbformat": 4
            },
            "nbformat": 4,
            "nbformat_minor": 2
        }
    },
    {
        "title": "Machine learning notes - Summary of Andrew's Ng course",
        "description": "A non-exhaustive course on machine learning, with a summary of Andrew Ng's course. Provides a good overview of the different machine learning algorithms and their applications, all along with formulas and examples.",
        "tags": [
            "Machine learning",
            "Regression",
            "Classification",
            "Neural networks"
        ],
        "date": "September 27, 2021",
        "id": "machine_learning",
        "body": {
            "cells": [
                {
                    "cell_type": "markdown",
                    "metadata": {},
                    "source": [
                        "# **Machine learning**\n",
                        "\n",
                        "### *Introduction*\n",
                        "- Machine learning helps to reproduce tasks made by humans and those we cannot program by hand. It is also used for data mining and self-customizing programs (Customizing preferences and data).\n",
                        "- ML gives the capability to machines to learn without being explicitly programmed.\n",
                        "- Types of ML :\n",
                        "  - Supervised learning: We're given a dataset with labels. In other words, we already know what our output should look like. Supervised learning is categorized into :\n",
                        "    1. Regression problems: Predicting output within a continuous domain of definition (Mapping to a continuous function).\n",
                        "    2. Classification problems: Predicting output in a discrete domain of definition (Mapping to discrete categories can be seen as a step function).\n",
                        "  - Unsupervised learning: We're given a dataset without knowing how our results should look and don't necessarily know the effect of each variable. A solution is usually to cluster data based on relationships among the variables. With unsupervised learning, there's no feedback on the prediction results.\n",
                        "  - Other: Reinforcement learning, recommender systems.\n",
                        "\n",
                        "# Linear regression :\n",
                        "- Given a training set, our primary goal is to learn a mapping function $h: x \\rightarrow y$ which can predict correctly (or approximately) a value of $y$ given $x$ as input ($x, y \\in \\mathbb{R}$).\n",
                        "- By default, we define the hypothesis function as follow : $h_{\\theta}(x) = \\theta_{0} + \\theta_{1}x$ where $\\theta_{i}$ are parameters.\n",
                        "- $h(x)$ is a function for univariate linear regression (with one variable as input).\n",
                        "Cost function measures the loss between the output of $h(x)$ and the ground truth value $y$.\n",
                        "- Most commonly used cost function for regression problems is the \"Mean squared error\" and is defined as follow :\n",
                        "$$ J(\\theta_{0}, \\theta_{1}) = \\frac{1}{2m} \\sum_1^m (\\hat{y} - y)^2 $$  \n",
                        "\n",
                        "- Where $m$ is the number of training examples and $\\hat{y}$ is the predicted output. We put $\\frac{1}{2}$ on the MSE to simplify the computation of the gradient descent. When derived, it cancels the square.\n",
                        "\n",
                        "#### 1. Gradient descent:\n",
                        "- We modify the values of the parameters by subtracting its gradient each time until we get a function $h$ that predicts well. In other words, finding a combination of parameters $\\theta_{0}, \\theta_{1}, ..., \\theta_{n}$ that minimizes the cost as much as possible. In some cases, we run into local minimums when dealing with complex functions; one possible solution is to restart the learning phase with another distribution of parameters.  \n",
                        "``Repeat until convergence:``\n",
                        "$$ \\theta_{j} := \\theta_{j} - \\eta \\times \\frac{\\partial}{\\partial \\theta_{j}}J(\\theta_{0}, \\theta_{1},...,\\theta{n}) $$  \n",
                        "Where $\\eta$ is the learning rate. This parameter has an impact on the training speed. A substantial value means faster learning but can sometimes cause oscillations and divergence. In the other case, if $\\eta$ is too small, then gradient descent will be slow.\n",
                        "- *Note*: All the parameters should be updated at once.\n",
                        "- One possible solution if the cost starts growing after $k$ iterations is to reduce the value of the learning rate $\\eta$.\n",
                        "If $\\eta$ is sufficiently small, the cost should decrease on every iteration, but the gradient descent will hardly converge.\n",
                        "- *Note*: Some other gradient-based optimization algorithms: Conjugate gradient, BFGS, L-BFGS (more complex).\n",
                        "\n",
                        "#### 2. Multivariate linear regression (Linear algebra):\n",
                        "- let $n$ be the number of input features. If $n > 1$, then we're working on multivariate linear regression problem :\n",
                        "$h_{\\theta}(x) = \\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2} + ... + \\theta_{n}x_{n}$\n",
                        "- By vectorizing our hypothesis function, we'll be able to reduce the computing time and apply gradient descent on all training examples at once. This method is called '**Batch gradient descent (BGD)**' (Update is made on a whole batch of examples).  \n",
                        "$$\\theta = \\begin{bmatrix}\\theta_{0} \\\\\\theta_{1} \\\\... \\\\\\theta_{n} \\end{bmatrix},\n",
                        "x = \\begin{bmatrix}x_{0}^{(1)} & x_{0}^{(2)} & ... & x_{0}^{(m)}\\\\x_{1}^{(1)} & x_{1}^{(2)} & ... & x_{1}^{(m)} \\\\. & . & . & . \\\\x_{n}^{(1)} & x_{n}^{(2)} & ... & x_{n}^{(m)} \\end{bmatrix}, \n",
                        "h_{\\theta}(x) = \\theta^{T}x$$  \n",
                        "\n",
                        "- In more details, here's what the **BGD** formula looks like : $\\theta_{j} := \\theta_{j} - \\frac{\\eta}{m} \\sum_i^m \\frac{\\partial}{\\partial \\theta_{j}}J^{(i)}(\\theta_{0}, \\theta_{1},..., \\theta_{n})$ where $\\frac{\\partial J}{\\partial \\theta_{j}}$ formula can be found as follow :  \n",
                        "$$\\frac{\\partial J}{\\partial \\theta_{j}} = \\frac{\\partial J}{\\partial h}\\frac{\\partial h}{\\partial \\theta_{j}} = (h_{\\theta}(x) - y)x_{j}$$\n",
                        "$$\\theta_{j} := \\theta_{j} - \\frac{\\eta}{m} \\sum_{i=1}^m (h_{\\theta}(x^{(i)}) - y^{(i)})x_{j}^{(i)}$$\n",
                        "\n",
                        "#### 3. Features scaling (z-score normalization):\n",
                        "- When dealing with multivariate regression problems, it is crucial to rescale all the features to the same range around 0, especially when we have different units. Variables measured at different scales do not contribute equally to the analysis and might create a bias. A variable that ranges between 0 and 1000 will outweigh a variable that ranges between 0 and 1.\n",
                        "- One way to rescal input features is to apply the formule :\n",
                        "$$ x_{i} := \\frac{x_{i} - \\mu_{i}}{s_{i}} $$  \n",
                        "Where $\\mu_{i}$ is the average value of the feature $x_{i}$ and $s_{i}$ is the standard deviation ``max - min``.\n",
                        "\n",
                        "#### 4. Normal equations :\n",
                        "- Another method to minimize the function $\\partial J$ without using an iterative algorithm like the gradient descent is to use the '**Normal equation**'. In this method, we minimize the cost by resolving the problem $\\frac{\\partial J}{\\partial \\theta} = 0$ and by simplifying this equation, we obtain the normal equation formula:\n",
                        "$$\\theta = (X^TX)^{-1}X^Ty$$ \n",
                        "- I explain in more details the demonstration of this function [here](https://github.com/Arksyd96/machine_learning/blob/main/normal_equation_demonstration.ipynb) ([NBviewer version](https://nbviewer.jupyter.org/github/Arksyd96/machine_learning/blob/main/normal_equation_demonstration.ipynb)).\n",
                        "- The normal equation has some advantages and disadvantages over the default method, which is gradient descent. Here are some points (From Andrew Ng's course):\n",
                        "  - **Gradient descent**:\n",
                        "    - Pros:\n",
                        "      - Complexity $O(kn^2)$.\n",
                        "      - Works well when the number of features is large.\n",
                        "    - Cons:\n",
                        "      - Features scaling is necessary.\n",
                        "      - Needs many iterations.\n",
                        "      - Needs to define a learning rate $\\eta$.\n",
                        "  - **Normal equation**:\n",
                        "    - Pros:\n",
                        "      - No iteration.\n",
                        "      - No need for a learning rate.      \n",
                        "    - Cost:\n",
                        "      - Computing inverse of $X^TX$ has a complexity of $O(n^3)$.\n",
                        "      - Slow when dealing with a lot of features\n",
                        "- *Note:* Features scaling is unnecessary when working with normal equations; keeping the features as they are won't affect the results.\n",
                        "- It is recommended to use **Normal equation** when the number of features $n$ does not exceed $10^{5 to 6}$; otherwise, gradient descent is better. \n",
                        "- *Note:* In some cases, the term $X^TX$ is noninvertible. This happens if:\n",
                        "  - There are redundant or/and linearly dependent features.\n",
                        "  - Too many features $n > m$.\n",
                        "\n",
                        "# Polynomial regression:\n",
                        "- Sometimes, the repartition of our training data forms a non-linear curve, and our hypothesis function cannot fit all these data. To deal with non-linear problems, we have to use polynomial regression.\n",
                        "- The idea behind polynomial regression is to compose our input features with non-linear functions then create new additional features based on those compositions. Here are some examples:\n",
                        "  - Quadratic function: $h_{\\theta}(x) = \\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{1}^2$\n",
                        "  - Cubic function: $h_{\\theta}(x) = \\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{1}^2 + \\theta_{3}x_{1}^3$\n",
                        "  - Root squared function: $h_{\\theta}(x) = \\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}\\sqrt{x_{1}}$\n",
                        "- One important thing to keep in mind is if you choose your features this way, then feature scaling becomes very important. Do not forget to normalize your inputs.\n",
                        "\n",
                        "# Classification:\n",
                        "- Given a training set, our goal is to predict to which class a training example belongs. Unlike the logistic regression hypothesis function, in classification, we predict discrete values $y \\in \\{0, 1, ..., n_{class}\\}$.\n",
                        "- When the number of output classes $n_{class}$ is equal to $2$, we call it a binary classification problem, and outputs are either positive ($1$) or negative ($0$). \n",
                        "- One approach is to use linear regression and map predictions greater than 0.5 as a 1 and lesser than 0.5 as a 0. However, this method doesn't work well because classification is not a linear function (By Andrew Ng). On possible alternative is to use **Logistic regression**.\n",
                        "\n",
                        "#### 1. Logistic regression:\n",
                        "- **Logistic regression** is the most commonly used technique for binary classification. Our hypothesis function needs to be modified because the default outputs value greater than 1 and lower than 0. In order to obtain $\\hat{y} \\in \\{0, 1\\}$ we will combine our hypothesis function with a logistic function to satisfy $0 \\leq h_{\\theta}(x) \\leq 1$. The output will then be interpreted as a probability.\n",
                        "$$ h_{\\theta}(x) = P(y = 1|x;\\theta) \\in [0, 1] $$\n",
                        "$$  h_{\\theta}(x) = \\sigma(\\theta^Tx), \\;\\; where \\;\\; \\sigma(x) = \\frac{1}{1 + e^{-x}} $$\n",
                        "- The function $\\sigma$ is also called sigmoid function. This function maps any real number to the interval $[0, 1]$ which makes it useful for binary classification.\n",
                        "- The sigmoid is a non-linear function, combined with $h_{\\theta}(x)$, it allows us solve complex problems that require non-linear functions.\n",
                        "\n",
                        "\n",
                        "\n",
                        "##### Decision boundary:\n",
                        "- Decision boundary (or decision surface) is a function that delimits the region of each class in the data space. If the decision surface is a hyperplane, then the classification problem is linear, and the classes are linearly separable ([Wikipedia](https://en.wikipedia.org/wiki/Linear_separability)).\n",
                        "- To find the decision boundary function of a binary classification problem:\n",
                        "$$ given \\;\\; h_{\\theta}(x) = \\sigma(z) \\;\\; where \\;\\; z = \\theta^Tx $$\n",
                        "$$ y = 1 \\;\\; if \\;\\; h_{\\theta}(x) \\geq 0.5 \\Longrightarrow z \\geq 0 $$\n",
                        "$$ y = 0 \\;\\; if \\;\\; h_{\\theta}(x) < 0.5 \\Longrightarrow z < 0 $$\n",
                        "$$ \\theta^Tx = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + ... + \\theta_nx_n = 0 $$\n",
                        "\n",
                        "- Unlike regression problems, we cannot use the 'Mean squared error' cost function because when combined with a sigmoid, the output function will be non-convex and, thus, will contain many local optima. Instead, we use the cross-entropy cost function, which looks like this:\n",
                        "$$ J(\\theta) = \\begin{cases}y=1 & -log(h_{\\theta}(x))\\\\y=0 & -log(1 - h_{\\theta}(x))\\end{cases}  $$\n",
                        "- If the model makes a mistake (predicts $1$ when $y = 0$ or predict $0$ when $y = 1$), the value of the cost will converge to $\\infty$.\n",
                        "- If the model makes a good prediction, the cost value will converge to $0$.\n",
                        "When combined with the sigmoid, the cost function will be convex (Function with a bowl form, with only 1 optimum).\n",
                        "- A more simplified way to write the cost function is to group the two terms of $J(\\theta)$ into one case:\n",
                        "    - $when \\;\\; y = 1 \\Longrightarrow J(\\theta) = -log(h_{\\theta}(x^{(i)}))$.\n",
                        "    - $when \\;\\; y = 0 \\Longrightarrow J(\\theta) = -log(1 - h_{\\theta}(x^{(i)}))$.\n",
                        "$$ J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m [y^{(i)} log(h_{\\theta}(x^{(i)})) + (1 - y^{(i)}) log(1 - h_{\\theta}(x^{(i)}))]$$\n",
                        "- This loss function can be derived from statistics using the principle of **Maximum likelihood estimation**. Which is an idea in statistics for how to efficiently find parameters data for different models. I explain more in details the demonstration of this function [here]()([NBviewer version]()).\n",
                        "- When deriving $J(\\theta)$ with respect to $\\theta_j$, we obtain the same function as in regression and which is convex:\n",
                        "$$ \\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^m (h_{\\theta}(x^{(i)}) - y^{(i)})x^{(i)} $$\n",
                        "- *Note*: Since the derivatives of cost functions for regression and logistic regression are the same. One may wonder what is the difference then? The difference is lies in the hypothesis function, where for logistic regression, is combined with a sigmoid $h_{\\theta}(x) = \\frac{1}{1 + e^{\\theta^Tx}}$.\n",
                        "- *Note*: Vectorized implementation for gradient descent:\n",
                        "$$ \\theta := \\theta - \\frac{\\eta}{m}X^T(\\sigma(X\\theta) - Y) $$\n",
                        "\n",
                        "#### 2. One-vs-all method for multiclass classification:\n",
                        "- A multiclass classification is a classification problem that satisfies $\\;n_{class} > 2$ and $y \\in \\{1, 2, ..., n_{class}\\}$.\n",
                        "- One-vs-all method consists on dividing a multiclass classification problem into $n_{class}$ logistic regression models. For each class, we denote its data as being the first class, and we group the rest of the data in the second one:\n",
                        "$$ prediction = arg\\;max(h_{\\theta}^{(0)}(x), h_{\\theta}^{(1)}(x), h_{\\theta}^{(2)}(x), ..., h_{\\theta}^{(n_{class})}(x)) $$ \n",
                        "- We'll pick the class whose hypothesis function offers the highest probability. \n",
                        "\n",
                        "# Overfitting and underfitting:\n",
                        "1. **Underfitting** (or high bias problem) is when our model maps poorly the data because our hypothesis function is too simple to fit complex problems or uses too few features. Try one of the below solutions :\n",
                        "    - Train the model for few more iterations.\n",
                        "    - Try bigger learning rate values.\n",
                        "    - Add extra features $(x^n, \\sqrt{x}$. etc $)$.\n",
                        "    - Add some training data.\n",
                        "2. **Overfitting** (or high variance problem) is a state where the model performs so well $(J(\\theta) \\simeq 0)$ on training data but fails to generalize when given new examples. We also call the problem as the **High variance** problem. Here are some approaches for solving the overfitting problem :\n",
                        "    - Restart training with fewer iterations.\n",
                        "    - Try smaller learning rate values.\n",
                        "    - Reduce the number of features by keeping only the important ones.\n",
                        "    - Use model selection algorithms.\n",
                        "    - Regularization.  \n",
                        "    \n",
                        "Check my article on [Bias-Variance tradeoff]() for more formal details about overfitting and underfitting.\n",
                        "\n",
                        "# Regularization:\n",
                        "- Regularization is a set of techniques that are used to prevent overfitting. There are many new ways to do regularization, we'll see here the simplest and most common one.\n",
                        "- Regularization allows to reduce the complexity of the model by adding a penalty term to the cost function. In this way, we can keep our polynomial features to fit complex problems without overfitting.\n",
                        "\n",
                        "Suppose we're overfitting with the following hypothesis function:\n",
                        "$$ h_{\\theta}(x) = \\theta_0 + \\theta_1x + \\theta_2x^2 + \\theta_3x^3 + \\theta_4x^4 $$  \n",
                        "The idea behind regularization is to reduces values of $\\theta_j$ of higher polynomial features in order to make the function *simpler* (By minimizing $\\theta_j$, we reduce the slope and the function will be more linear).  \n",
                        "Here's the loss function with regularization:\n",
                        "$$ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m (h_{\\theta}(x^{(i)}) - y^{(i)})^2 + \\frac{\\lambda}{2m} \\sum_{j=1}^n \\theta_j^2 $$\n",
                        "Where $\\lambda$ is the regularization parameter and is generally set to a small value (e.g. $\\lambda = 0.1$). The value of the hyperparameter $\\lambda$ is crucial, it has a direct impact on the balance between the cost function and the regularization term, those two terms are combined in the cost function and should be tuned accordingly.  "
                    ]
                }
            ],
            "metadata": {
                "language_info": {
                    "name": "python"
                },
                "orig_nbformat": 4
            },
            "nbformat": 4,
            "nbformat_minor": 2
        }
    },
    {
        "title": "Demonstration of normal equation formula for regression",
        "description": "A detailed step-by-step demonstration of the normal equation formula for regression.",
        "tags": ["Mathematics", "Regression"],
        "date": "September 30, 2021",
        "id": "normal_equation_demonstration",
        "body": {
            "cells": [
                {
                    "cell_type": "markdown",
                    "metadata": {},
                    "source": [
                        "# Demonstration of the normal equation formula:\n",
                        "0. *We define the hypothesis and cost function as follow*:\n",
                        "$$J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m (h_{\\theta}(x^{(i)}) - y^{(i)})^2$$ \n",
                        "$$ \\theta = \\begin{bmatrix}\\theta_{0} \\\\\\theta_{1} \\\\... \\\\\\theta_{n} \\end{bmatrix},\n",
                        "x = \\begin{bmatrix}x_{0}^{(1)} & x_{0}^{(2)} & ... & x_{0}^{(m)}\\\\x_{1}^{(1)} & x_{1}^{(2)} & ... & x_{1}^{(m)} \\\\. & . & . & . \\\\x_{n}^{(1)} & x_{n}^{(2)} & ... & x_{n}^{(m)} \\end{bmatrix}, \n",
                        "h_{\\theta}(x) = \\theta^{T}x$$  \n",
                        "\n",
                        "- *Note*: Our objective is to find the value of $\\theta$ that solves the equation $\\frac{\\partial J}{\\partial \\theta} = 0$.\n",
                        "1. First we start by simplyfing the formula:\n",
                        "    - Remove the $\\frac{1}{2m}$ since it has no impact when deriving.\n",
                        "    - Eliminate the sum by introducing $(h - y)$ as a design matrix (Matrix that contains everything, it's shape will be $(n_{features}, m)$).\n",
                        "$$J(\\theta) = (\\begin{bmatrix} \\theta^Tx^{(1)} \\\\ \\theta^Tx^{(2)} \\\\ ... \\\\ \\theta^Tx^{(m)} \\end{bmatrix} - \\begin{bmatrix} y^{(1)} \\\\ y^{(2)} \\\\ ... \\\\ y^{(m)} \\end{bmatrix})^2$$ \n",
                        "$$J(\\theta) = (\\begin{bmatrix} \\theta_0 x_0^{(1)} & \\theta_1 x_1^{(1)} & ... & \\theta_{n} x_n^{(1)} \\\\ \\theta_{0} x_0^{(2)} & \\theta_1x_1^{(2)} & ... & \\theta_{n} x_n^{(2)} \\\\ . & . & . & . \\\\ \\theta_0x_0^{(m)} & \\theta_1x_1^{(m)} & ... & \\theta_{n} x_n^{(m)}\\end{bmatrix} - \\begin{bmatrix}y^{(1)}\\\\y^{(2)}\\\\...\\\\y^{(m)}\\end{bmatrix})^2$$\n",
                        "- *Note* that dimension of $x$ written this way becomes $m \\times n$ instead of $n \\times m$. This allows use to rewrite it in a more general form:\n",
                        "$$J(\\theta) = (X\\theta - Y)^2$$\n",
                        "2. Deploy the binomial square identity (Be careful, this is the matrix version so we have to transpose the first term in order to square all the numbers):\n",
                        "\n",
                        "$$J(\\theta) = (X\\theta - Y)^T(X\\theta - Y) = (X\\theta)^TX\\theta - (X\\theta)^TY - Y(X\\theta)^T + Y^TY$$\n",
                        "3. Sum the $2nd$ and $3rd$ terms, $(X\\theta)^T$ and $Y$ are vectors. if $A$ and $B$ are vector matrices, then $A^TB = B^TA$ (Take care of this information, you'll need it a lot when dealing with linear algebra in machine learning).\n",
                        "$$J(\\theta) = (X\\theta)^TX\\theta - 2(X\\theta)^TY + Y^TY $$\n",
                        "4. The formula as it is now cannot be more simplified. So we'll start searching for it's derivative with respect to $\\theta$:\n",
                        "$$ \\frac{\\partial J}{\\partial \\theta} = \\frac{\\partial (X\\theta)^TX\\theta}{\\partial \\theta} - \\frac{\\partial 2(X\\theta)^TY}{\\partial \\theta} + \\frac{\\partial Y^TY}{\\partial \\theta} $$\n",
                        "$$ \\frac{\\partial Y^TY}{\\partial \\theta} = 0 $$\n",
                        "$$ \\frac{\\partial 2(X\\theta)^TY}{\\partial \\theta} = 2X^TY$$ \n",
                        "$$ \\frac{\\partial (X\\theta)^TX\\theta}{\\partial \\theta} = X^TX\\theta + X(X\\theta)^T = 2X^TX\\theta$$\n",
                        "5. The initial objective was to resolve $\\frac{\\partial J}{\\partial \\theta} = 0$, so we put each term together:\n",
                        "$$ \\frac{\\partial J}{\\partial \\theta} = 2X^TX\\theta - 2X^TY = 0$$\n",
                        "$$ 2X^TX\\theta = 2X^TY $$\n",
                        "$$ X^TX\\theta = X^TY $$\n",
                        "6. And finally, the fraction $\\frac{1}{A}$ where $A$ is a matrix, is equivalent to the inverse of $A$. The inverse of a matrix is noted $A^{-1}$ and $AA^{-1} = I$.\n",
                        "$$ \\theta = (X^TX)^{-1}X^TY $$\n"
                    ]
                },
                {
                    "cell_type": "markdown",
                    "metadata": {},
                    "source": []
                }
            ],
            "metadata": {
                "language_info": {
                    "name": "python"
                },
                "orig_nbformat": 4
            },
            "nbformat": 4,
            "nbformat_minor": 2
        }
    }
]
