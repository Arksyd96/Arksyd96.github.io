(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[195],{659:function(e,n,t){"use strict";t.r(n),t.d(n,{default:function(){return A}});var r,i,a=t(66311),o=t(67294),s=t(52986),l=t(18611),$=t(94022),c=t(87121),u=t(9888),d=t(42619),p=t(52209),m=t(22636),b=t(46329),f=t(84283),h=t(25935),x=t(85893),v=t(9980)(),g=t(26649);v.use(g,{throwOnError:!1,errorColor:" #cc0000"});var P=m.Z.div(r||(r=(0,p.Z)(["\n    display: flex;\n    flex-direction: column;\n    color: black;\n    background-color: rgba(220, 220, 220, 0.88);\n    box-shadow: 0px 0px 4px 4px rgba(150, 150, 150, 0.35);\n    backdrop-filter: blur(6px);\n    width: 100%;\n    border-radius: 5px;\n    padding: 2vh 4vw;\n    box-sizing: border-box;\n    margin: 15vh 0 10vh 0;\n"]))),_={backgroundColor:"rgba(255, 255, 255, 0.8)",borderRadius:"5px",fontSize:"1em",margin:"0 2vw 0 2vw"},X=m.Z.div(i||(i=(0,p.Z)(["\n    display: flex;\n    justify-content: center;\n    margin: 2vh 2vw 0 2vw;\n"]))),y=t(64897),j=function(){return(0,x.jsx)(P,{children:y.cells.map((function(e,n){return"markdown"===e.cell_type?(0,x.jsx)("div",{children:(0,h.ZP)(v.render(e.source.join("")))},n):(0,x.jsxs)(o.Fragment,{children:[(0,x.jsx)(b.Z,{showLineNumbers:!0,language:"python",style:f.vs,customStyle:_,children:e.source.join("")},n),e.outputs.map((function(e,n){return"display_data"===e.output_type?(0,x.jsx)(X,{children:(0,x.jsx)("img",{src:"data:image/png;base64,"+e.data["image/png"],style:{width:"100%"}},n)},n):(0,x.jsx)(b.Z,{language:"cal",style:f.YC,customStyle:{margin:"2vh 2vw 0 2vw",borderRadius:"5px",backgroundColor:"rgba(0, 0, 0, 0.5)"},children:e.text.join("")},n)}))]},n)}))})},w=t(71472),A=function(){var e=o.useState(!0),n=(0,a.Z)(e,2),t=n[0],r=n[1];return o.useEffect((function(){return console.log("loaded")}),[]),(0,x.jsxs)("div",{className:"App",children:[(0,x.jsx)(s.Z,{blog:!0}),(0,x.jsxs)(l.Z,{blog:!0,children:[(0,x.jsx)(c.Z,{}),(0,x.jsx)(u.Z,{enableParticles:t}),(0,x.jsx)(w.Z,{checked:t,onClick:function(){return r(!t)}}),(0,x.jsx)(d.Z,{id:"blog-homepage",offset:0,minHeight:"90vh",children:(0,x.jsx)(j,{})})]}),(0,x.jsx)($.Z,{})]})}},9888:function(e,n,t){"use strict";var r,i=t(92809),a=t(52209),o=t(67294),s=t(22636),l=t(85893);function $(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);n&&(r=r.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,r)}return t}var c=s.Z.canvas(r||(r=(0,a.Z)(["\n    position: fixed;\n    top: 0;\n    left: 0;\n    z-index: -1;\n    width: 100%;\n    min-height: 100vh;\n    background-image: linear-gradient(to bottom,  black 20%, rgb(87, 111, 116));\n"])));n.Z=function(e){var n=(0,o.useRef)(null),t=null,r=null,a=[],s=null,u=!1,d=function(){var e=.5*(Math.random()+1);return{x:Math.random()*t.width,y:Math.random()*t.height,radius:2*e+1.5,opacity:e,color:"#aaa",velocity:{x:30*(Math.random()-.5),y:30*(Math.random()-.5)}}},p=Date.now(),m=function e(){r.clearRect(0,0,t.width,t.height),r.globalAlpha=1;var n=Date.now(),i=n-p;p=n,t&&(a.forEach((function(e,n){!function(e,n){(e.x>t.width||e.x<0)&&(e.velocity.x=-e.velocity.x),(e.y>t.height||e.y<0)&&(e.velocity.y=-e.velocity.y),e.x+=e.velocity.x*n,e.y+=e.velocity.y*n}(e,i/1e3),function(e){r.beginPath(),r.arc(e.x,e.y,e.radius,0,2*Math.PI),r.fillStyle=e.color,r.globalAlpha=e.opacity,r.fill()}(e),function(e,n){for(var t=n+1;t<a.length;t++){var i=a[t],o=Math.abs(i.x-e.x),s=Math.abs(i.y-e.y),l=Math.sqrt(o*o+s*s),$=e.radius+i.radius;l<=110&&l>$&&(r.beginPath(),r.strokeStyle=e.color,r.globalAlpha=(110-l)/110*e.opacity*i.opacity,r.lineWidth=.7,r.moveTo(e.x,e.y),r.lineTo(i.x,i.y),r.stroke())}}(e,n)})),requestAnimationFrame(e))},b=function(e){s.x=e.clientX+e.clientX/t.width*10,s.y=e.clientY},f=(0,o.useState)({width:0,height:0}),h=f[0],x=f[1];return(0,o.useEffect)((function(){function e(){x({width:window.innerWidth,height:window.innerHeight})}return window.addEventListener("resize",e),e(),function(){return window.removeEventListener("resize",e)}}),[]),(0,o.useEffect)((function(){return t=n.current,r=t.getContext("2d"),u||(!function(){for(var n=e.enableParticles?t.width*t.height/6e3:0,r=0;r<n;r++){var i=d();a.push(i)}(s=d()).color="orange",s.redius=2.5,s.opacity=.7,s.velocity.x=0,s.velocity.y=0,a.push(s)}(),u=!0),requestAnimationFrame(m),window.addEventListener("mousemove",b),function(){return window.removeEventListener("mousemove",b)}}),[h,e.enableParticles]),(0,l.jsx)(c,function(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?$(Object(t),!0).forEach((function(n){(0,i.Z)(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):$(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}({width:h.width,height:h.height,ref:n},e))}},87121:function(e,n,t){"use strict";var r,i,a=t(52209),o=(t(67294),t(22636)),s=t(17625),l=t(51417),$=t(51436),c=t(85893),u=o.Z.div({position:"fixed",right:"15px",top:"40%",display:"flex",flexDirection:"column"}),d=o.Z.a(r||(r=(0,a.Z)(["\n    display: flex;\n    align-items: center;\n    justify-content: center;\n    font-size: 24px;\n    width: 40px;\n    height: 40px;\n    border: 1px solid #FFFFFF;\n    text-decoration: none;\n    margin: 5px 5px;\n    background: transparent;\n    border-radius: 100%;\n    cursor: pointer;\n    transition: background 0.1s ease-in;\n    :hover {\n        background: orange;\n        border: 1px solid orange;\n    }\n    @media (max-width: 768px) {\n        display: none;\n    }\n"]))),p=(0,o.Z)(s.G)(i||(i=(0,a.Z)(["\n    color: #FFFFFF;\n"])));n.Z=function(){var e=[{href:"mailto:aghiles.kebaili.1998@gmail.com",icon:$.FU$},{href:"https://www.linkedin.com/in/aghiles-kebaili/",icon:l.D9H},{href:"https://github.com/Arksyd96",icon:l.zhw}];return(0,c.jsx)(u,{children:e.map((function(e,n){return(0,c.jsx)(d,{href:e.href,target:0!==n?"_blank":"",children:(0,c.jsx)(p,{icon:e.icon})},n)}))})}},71472:function(e,n,t){"use strict";var r,i=t(52209),a=(t(67294),t(22636)),o=t(85893),s=a.Z.input(r||(r=(0,i.Z)(['\n    appearance: none;\n    cursor: pointer;\n    :focus {\n        outline: none;\n    }\n    height: 32px;\n    width: 52px;\n    border-radius: 16px;\n    display: inline-block;\n    position: fixed;\n    top: 12vh;\n    right: 15px;\n    margin: 0;\n    border: 2px solid #474755;\n    background: white;\n    transition: all 0.2s ease;\n    :after {\n        content: "";\n        position: absolute;\n        top: 2px;\n        left: 2px;\n        width: 24px;\n        height: 24px;\n        border-radius: 50%;\n        background: black;\n        box-shadow: 0 1px 2px rgba(44, 44, 44, 0.2);\n        transition: all 0.2s cubic-bezier(0.5, 0.1, 0.75, 1.35);\n    }\n    :checked {\n        border-color: orange;\n        background-color: orange;\n        :after {\n            background: white;\n            transform: translatex(20px);\n        }\n    }\n'])));n.Z=function(e){return(0,o.jsx)(s,{type:"checkbox",checked:e.checked,onClick:e.onClick})}},94022:function(e,n,t){"use strict";var r,i=t(52209),a=(t(67294),t(22636)),o=t(85893),s=a.Z.footer(r||(r=(0,i.Z)(["\n    display: flex;\n    width: 100%;\n    height: 10vh;\n    background: transparent;\n    backdrop-filter: blur(4px);\n    color: orange;\n    box-shadow: rgba(50, 50, 93, 0.25) 0px -13px 27px -5px, rgba(80, 80, 80, 0.3) 0px -8px 16px -8px;\n    align-items: center;\n    justify-content: center;\n"])));n.Z=function(e){return(0,o.jsx)(s,{children:"Designed and built with \u2764\ufe0f by Kebaili Aghiles."})}},52986:function(e,n,t){"use strict";var r,i,a=t(66311),o=t(52209),s=t(22636),l=t(41664),$=t(67294),c=t(85893),u=s.Z.header((function(e){return"\n    position: fixed;\n    top: 0;\n    display: flex;\n    align-items: center;\n    justify-content: space-between;\n    flex-direction: row;\n    width: 100%;\n    height: 10vh;\n    padding: 0 10%;\n    box-sizing: border-box;\n    align-self: center;\n    transition: all 0.5s ease;\n    backdrop-filter: blur(6px);\n    box-shadow: rgba(50, 50, 93, 0.25) 0px 13px 27px -5px,\n        rgba(80, 80, 80, 0.3) 0px 8px 16px -8px;\n    background-color: rgba(20, 20, 20, 0.5);\n    z-index: 11;\n    @media (max-width: 768px) {\n        padding: 0 5%;\n        height: 6vh;\n    }\n"})),d=s.Z.ul({display:"flex",listStyle:"none",flexDirection:"row"}),p=s.Z.div(r||(r=(0,o.Z)(["\n    display: flex;\n    width: 8%;\n    height: 50%;\n    background-repeat: no-repeat;\n    background-size: contain;\n    background-position: center;\n    cursor: pointer;\n"]))),m=s.Z.a(i||(i=(0,o.Z)(['\n    color: #FFFFFF;\n    text-decoration: none;\n    font-weight: bold;\n    transition: width 0.5s ease, color 0.5s ease;\n    cursor: pointer;\n    margin-left: 30px;\n    :hover {\n        color: orange;\n    }\n    :before {\n        color: orange;\n        content: "','";\n    }\n'])),(function(e){return e.number+". "}));n.Z=function(e){var n=$.useState(!1),t=(0,a.Z)(n,2),r=t[0],i=t[1];(0,$.useEffect)((function(){window.innerWidth<768&&i(!0)}),[]);return(0,c.jsxs)(u,{children:[(0,c.jsx)(p,{className:"logo-container",onClick:function(){window.scrollTo(0,0)}}),(0,c.jsx)("nav",{children:(0,c.jsx)(d,{children:e.blog?r?null:(0,c.jsx)(l.default,{href:"/",children:(0,c.jsx)(m,{number:"1",children:"Home"})}):r?null:(0,c.jsxs)($.Fragment,{children:[(0,c.jsx)(m,{number:"1",href:"#curriculum",children:"Curriculum"}),(0,c.jsx)(m,{number:"2",href:"#projects",children:"Projects"}),(0,c.jsx)(m,{number:"3",href:"#contact",children:"Contact"}),(0,c.jsx)(l.default,{href:"/blog",children:(0,c.jsx)(m,{number:"4",children:"Blog"})})]})})})]})}},18611:function(e,n,t){"use strict";var r,i=t(52209),a=(t(67294),t(22636)),o=t(85893),s=a.Z.main(r||(r=(0,i.Z)(["\n    margin: ",";\n    box-sizing: border-box;\n"])),(function(e){return e.blog?"0 18%":"0 10%"}));n.Z=function(e){return(0,o.jsx)(s,{blog:e.blog,children:e.children})}},42619:function(e,n,t){"use strict";var r,i=t(52209),a=t(67294),o=t(22636),s=t(85893),l=o.Z.section(r||(r=(0,i.Z)(["\n    display: flex;\n    flex-direction: column;\n    justify-content: center;\n    position: relative;\n    top: ",";\n    color: #ffffff;\n    min-height: ",";\n    opacity: 0;\n    transition: all 0.6s ease-in-out;\n    @media (max-width: 768px) {\n        top: ",";\n        min-height: ",";\n        margin-top: 4vh;\n    }\n"])),(function(e){return e.invert?"-50px":"50px"}),(function(e){return e.minHeight}),(function(e){return e.invert?"-50px":"200px"}),(function(e){return e.minHeight})),$={top:0,opacity:1};n.Z=function(e){var n=(0,a.useState)(!e.apply),t=n[0],r=n[1],i=(0,a.useCallback)((function(){var n=window.innerWidth<768;window.scrollY>=e.offset-(n?200:0)&&r(!0)}));return(0,a.useEffect)((function(){return"home"===e.id&&r(!0),window.addEventListener("scroll",i),function(){window.removeEventListener("scroll",i)}})),(0,s.jsx)(l,{style:t?$:null,invert:e.invert,minHeight:e.minHeight,id:e.id,children:e.children})}},55809:function(e,n,t){(window.__NEXT_P=window.__NEXT_P||[]).push(["/blog",function(){return t(659)}])},64897:function(e){"use strict";e.exports=JSON.parse('{"cells":[{"cell_type":"markdown","source":["(R\xe9f\xe9rence des formules: Bibm@th)\\r\\n","# **Les variables al\xe9atoires** \\r\\n","## **1. Variables al\xe9atoires discr\xe8tes :**\\r\\n","Une variable al\xe9atoire discr\xe8te est une application $X$ de $\\\\Omega$ (Univers, contenant toutes les possibilit\xe9s) dans $E$ tel que $X(\\\\Omega)$ soit fini ou d\xe9nombrable.\\r\\n","- Notons : $X(\\\\Omega) = \\\\{x_n; n \\\\in I\\\\}$ o\xf9 $I$ est fini ou d\xe9nombrable.\\r\\n","- La loi de probabilit\xe9 de $X$, soit $(P_n)_{n \\\\; \\\\in \\\\; I}$, est not\xe9 : $P_n =P(X=x_n)$.\\r\\n","\\r\\n","**Note**: Un ensemble d\xe9nombrable est un ensemble en bijection avec l\'ensemble des entiers $\\\\mathbb{N}$ (\xe9quipotent).\\r\\n","\\r\\n","- La probabilit\xe9 $P(y|x)$ est la probabilit\xe9 conditionnelle de $y$ sachant $x$ (Probabilit\xe9 d\'un \xe9venement sachant qu\'un autre \xe9venement a eu lieu), et est d\xe9finit comme suit :\\r\\n","$$ P(Y = y|X = x) = \\\\frac{P(X = x, Y = y)}{P(X = x)}$$\\r\\n","o\xf9 $P(X = x, Y = y)$ est l\'application de la **loi conjointe** sur les variables $X$ et $Y$. \\r\\n","- La loi conjointe entre deux variables $X$ et $Y$ est la loi qui d\xe9finit toutes les probabilit\xe9s du vecteur $(X, Y)$ dans un ensemble $\\\\mathbb{E}^2$, en d\'autres termes, la probabilit\xe9 de l\'apparition de $X$ et $Y$ en m\xeame temps $P((X = x) \\\\cap (Y = y))$ (\xe0 ne pas confondre avec la probabilit\xe9 conditionnelle) :\\r\\n","$$ P_{X,Y}(I \\\\times J) = P(X \\\\in I \\\\; \\\\; et \\\\; \\\\; Y \\\\in J) $$  \\r\\n","***Exemple***: Soit $E$ un \xe9venement o\xf9 on tire 2 valeurs dans l\'univers $\\\\{-1, 1\\\\}^2$. $X$ est la somme des deux valeurs et $Y$ leur produit. On aura alors $X \\\\in \\\\{-2, 0, 2\\\\}$ et $Y \\\\in \\\\{-1, 1\\\\}$. Voici les valeurs possible de la loi conjointe entre $X$ et $Y$ :\\r\\n","$$ P(X = -2, Y = -1) = 0, \\\\;\\\\;\\\\;\\\\; P(X = 0, Y = -1) = \\\\frac{1}{2}$$\\r\\n","$$ P(X = 2, Y = -1) = 0, \\\\;\\\\;\\\\;\\\\; P(X = -2, Y = 1) = \\\\frac{1}{4}$$\\r\\n","$$ P(X = 0, Y = 1) = 0, \\\\;\\\\;\\\\;\\\\; P(X = 2, Y = 1) = \\\\frac{1}{4}$$  \\r\\n","**Note**: faites attention \xe0 l\'ordre des variables dans la loi conjointe.  \\r\\n","**Note**: Si $P(X = x, Y = y)$ est appell\xe9 loi conjointe, alors $P(X = x), P(Y = y)$ sont appell\xe9es **lois marginales**.  \\r\\n","- Il possible de retrouver les lois marginales \xe0 partir de la loi conjointe en fixant la valeur d\'une variable $X \\\\in \\\\{x_1, ..., x_p\\\\}$ ou $Y \\\\in \\\\{y_1, ..., y_q\\\\}$:\\r\\n","$$ P(X = x_i) = \\\\sum_{j=1}^q P(X = x_i, Y = y_j) \\\\;\\\\; et \\\\;\\\\; P(Y = y_j) = \\\\sum_{i=1}^p P(X = x_i, Y = y_j)$$ \\r\\n","- Cependant l\'inverse n\'est pas toujours correcte, il est possible de trouver la loi conjointe entre deux variables seulement si ces d\xe9rni\xe8res sont ind\xe9pendentes, et dans ce cas l\xe0 on aura :\\r\\n","$$ P(X = x, Y = y) = P(X=x)P(Y=y)$$  \\r\\n","Si les deux variables ne sont pas ind\xe9pentes alors il faudra plus d\'informations sur le rapport entre $X$ et $Y$ :\\r\\n","$$ P(X = x, Y= y) = P(X = x)P(Y = y | X = x) \\\\;\\\\; (origine \\\\;\\\\; de \\\\;\\\\; la \\\\;\\\\; loi \\\\;\\\\; conditionnelle)$$\\r\\n","\\r\\n","**I. Esp\xe9rence d\'une variable al\xe9atoire discr\xe8te:** not\xe9e $\\\\mathbb{E}[X]$, est la valeur moyenne qu\'on s\'attend \xe0 trouver pour la variable al\xe9atoire $X$. La formule de l\'esp\xe9rence est d\xe9finit comme suit :\\r\\n","$$ \\\\mathbb{E}(X) = \\\\sum_{n \\\\in I} x_n P(X = x_n)$$\\r\\n","**II. Variance d\'une variable al\xe9atoire discr\xe8te:** not\xe9e $V[X]$, la mesure de dispersion des valeurs de $X$. Elle est d\xe9finit comme suit :\\r\\n","$$ V[X] = \\\\mathbb{E}[(X - \\\\mathbb{E}[X])^2] = \\\\mathbb{E}[X^2] - \\\\mathbb{E}[X]^2$$\\r\\n","- *D\xe9monstration* de $V[X] \\\\Longrightarrow$ :  \\r\\n","$= \\\\mathbb{E}[(X - \\\\mathbb{E}[X])^2] = \\\\mathbb{E}[X^2 - 2X\\\\mathbb{E}[X] + \\\\mathbb{E}[X]^2]$  \\r\\n","$= \\\\mathbb{E}[X^2] - 2\\\\mathbb{E}[X]\\\\mathbb{E}[X] + \\\\mathbb{E}[X]^2 = \\\\mathbb{E}[X^2] - 2\\\\mathbb{E}[X]^2 + \\\\mathbb{E}[X]^2$  \\r\\n","$= \\\\mathbb{E}[X^2] - \\\\mathbb{E}[X]^2$   \\r\\n","**Note** $\\\\mathbb{E}[\\\\mathbb{E}[X]] = \\\\mathbb{E}[X]$  \\r\\n","\\r\\n","## **2. Variables al\xe9atoires continue (Variables \xe0 densit\xe9):**\\r\\n","Une variable al\xe9atoire continue est une fonction $X$ de $\\\\Omega$ dans $\\\\mathbb{R}$ $(X:\\\\Omega \\\\rightarrow \\\\mathbb{R})$ telle qu\'il existe $f: \\\\mathbb{R} \\\\rightarrow \\\\mathbb{R}$ continue par morceaux et v\xe9rifiant :\\r\\n","$$ \\\\forall a < b : P(X \\\\in [a, b]) = \\\\int_a^b f(x)dx$$\\r\\n","- Si une telle fonction existe, elle est alors appell\xe9 densit\xe9 de la variable al\xe9atoire $X$. Cette fonction doit n\xe9c\xe9ssairement \xeatre positive et v\xe9rifie $\\\\int_{- \\\\infty}^{+ \\\\infty} f(x)dx = 1$ (l\'air de la fonction $f$ correspond \xe0 une probabilit\xe9, et la somme des probabilit\xe9s de tout les \xe9venements $x_n$ dans $\\\\mathbb{R}$ doit \xeatre \xe9gale \xe0 1).\\r\\n","- On pose alors la loi suivante :\\r\\n","$$ f(x) = \\\\frac{dF(x)}{dx} $$\\r\\n","o\xf9 $F(x)$ est la fonction de r\xe9partition de la variable. \\r\\n","- Toutes variables continue est d\xe9finit par une certaine fonction de r\xe9partition, et en int\xe9grant cette d\xe9rin\xe8re on obtient la fonction de densit\xe9.\\r\\n","\\r\\n","**1. Esp\xe9rence d\'une variable al\xe9atoire continue**: de la m\xeame mani\xe8re que pour les variables discr\xe8tes, sauf qu\'on utilise une int\xe9grale \xe9tant donn\xe9 la continuit\xe9 de la fonction :\\r\\n","$$ \\\\mathbb{E}[X] = \\\\int_{-\\\\infty}^{+\\\\infty} xf(x)dx $$\\r\\n","**2. Variance d\'une variable al\xe9atoire continue** m\xeame formule : $ V[X] = \\\\mathbb{E}[X^2] - \\\\mathbb{E}[X]^2$\\r\\n","\\r\\n","## **3. Fonctions de r\xe9partition les plus connues :**\\r\\n","### **1. Loi g\xe9om\xe9trique :**\\r\\n","Une variable al\xe9atoire discr\xe8te $X$ suit une loi g\xe9om\xe9trique de param\xe8tres $p$, qu\'on note $X \\\\sim \\\\mathcal{G}(p)$ si :\\r\\n","- $X(\\\\Omega) = \\\\mathbb{N}^{*}$\\r\\n","- $P(X = k) = p(1 - p)^{k - 1} = pq^{k - 1}$  \\r\\n","\\r\\n","$X$ admet alors une esp\xe9rence et une variance :\\r\\n","- $\\\\mathbb{E}[X] = \\\\frac{1}{p}$  \\r\\n","- $V[X] = \\\\frac{q}{p^2} = \\\\frac{1 - p}{p^2}$ \\r\\n","\\r\\n","**Exemple**: Lancer de pi\xe8ce de monnaie truqu\xe9e avec les probabilit\xe9s $P(pile) = p$ et $P(face) = 1 - p = q$. soit $X$ le nombre de lancers pour obtenir pile pour la premi\xe8re fois, $X$ suit alors une loi g\xe9om\xe9trique. Dans cet exemple, le param\xe8tre $k$ est le nombre de lancers total.  \\r\\n","Pour $k = 5$ (5 lancers), la probabilit\xe9 d\'avoir pile pour la premi\xe8re fois est $pqqqq = pq^4$. L\'esp\xe9rence dans ce cas et la probabilit\xe9 moyenne d\'avoir pile, avec $k = 5$ on obtient $\\\\mathbb{E}[X = 5] = \\\\frac{1}{5} = 20\\\\%$ de chances.\\r\\n","\\r\\n","### **2. Loi de Poisson :**\\r\\n","Soit $\\\\lambda > 0$. Une variable al\xe9atoir discr\xe8te $X$ suit une loi de Poisson de param\xe8tre $\\\\lambda$ et not\xe9 $X \\\\sim \\\\mathcal{P}(\\\\lambda)$ si :\\r\\n","- $X(\\\\Omega) = \\\\mathbb{N}$\\r\\n","- $P(X = k) = e^{-\\\\lambda}\\\\frac{\\\\lambda^{k}}{k!}$\\r\\n","\\r\\n","$X$ admet alors une esp\xe9rence et une variance :\\r\\n","- $\\\\mathbb{E}[X] = \\\\lambda$\\r\\n","- $V[X] = \\\\lambda$\\r\\n","\\r\\n","*Note* la loi de Poisson est la loi des ph\xe9nom\xe8nes rares et des petites probabilit\xe9s, g\xe9n\xe9ralement utiliser pour mod\xe9liser des syst\xe8mes de file d\'attente. Pour plus d\'informations, consulter ce [lien](https://fr.wikipedia.org/wiki/Loi_de_Poisson).\\r\\n","\\r\\n","### **3.Loi exponentielle :**\\r\\n","$X$ est une variable al\xe9atoire continue suivant la loi exponentielle de param\xe8tre $a > 0$, note $X \\\\sim \\\\mathcal{e}(a)$ si elle est absolument continue et admet pour densit\xe9 :\\r\\n","- $f(x) = x =\\\\begin{cases}ae^{-ax} & si \\\\;\\\\; x > 0\\\\\\\\0 & sinon\\\\end{cases}$  \\r\\n","\\r\\n","$X$ admet une esp\xe9rence et une variance :\\r\\n","- $\\\\mathbb{E}[X] = \\\\frac{1}{a}$\\r\\n","- $V[X] = \\\\frac{1}{a^2}$\\r\\n","\\r\\n","*Note* la loi exponentielle est la version continue de la loi g\xe9om\xe9trique; Sert souvent \xe0 mod\xe9liser la dur\xe9e de vie d\'une entit\xe9. $X$ dans ce cas l\xe0 est une variable continue sans m\xe9moire, et elle v\xe9rifie donc :$\\\\forall x, y \\\\geq 0$ : $P(X \\\\geq x+y |X \\\\geq y) = P(X \\\\geq x)$ (car on traite g\xe9n\xe9ralement des p\xe9riodes de temps avec la loi exponentielle).  \\r\\n","\\r\\n","La r\xe9partition de $X$ est : $F(t) = \\\\begin{cases}0 & si \\\\;\\\\; t \\\\leq 0\\\\\\\\ 1 - e^{-at} & sinon \\\\end{cases}$  \\r\\n","Je note bien $t$ et non pas $x$ car l\'unit\xe9 est le temps.\\r\\n","\\r\\n","### **4. Loi uniforme :**\\r\\n","$X$ est une variable al\xe9atoire continue r\xe9partit de mani\xe8re uniforme sur un ensemble $[a, b]$, not\xe9 $X \\\\sim \\\\mathcal{U}(a, b)$ si elle admet comme fonction de densit\xe9 :\\r\\n","- $f(x) = \\\\begin{cases} \\\\frac{1}{b - a} & x \\\\in [a, b]\\\\\\\\0 & sinon \\\\end{cases}$  \\r\\n","\\r\\n","et la fonction de r\xe9partition de $X$ est alors :\\r\\n","- $F(x) = \\\\begin{cases} 0 & si \\\\;\\\\; t \\\\leq x \\\\\\\\ \\\\frac{x - a}{b - a} & si \\\\;\\\\; x \\\\in [a, b] \\\\\\\\ 1 & sinon\\\\end{cases}$\\r\\n","\\r\\n","On donne aussi : $\\\\mathbb{E}[X] = \\\\frac{a+b}{2}$ et $V[X] = \\\\frac{(b-a)^2}{12}$.\\r\\n","\\r\\n","### **5. Loi normale (ou Gaussienne) :** (Important pour le machine learning :D)\\r\\n","$X$ est une variable al\xe9atoire continue dont la r\xe9parition est normale (ou Gaussienne) avec les param\xe8tres $m$ (esp\xe9rence) et $\\\\sigma^2$ (variance), et que l\'on note $X \\\\sim \\\\mathcal{N}(m, \\\\sigma^2)$, si elle est continue et a pour densit\xe9 :\\r\\n","- $f(x) = \\\\frac{1}{\\\\sigma \\\\sqrt{2 \\\\pi}}exp(-\\\\frac{(x - m)^2}{2\\\\sigma^2})$\\r\\n","\\r\\n","On aura alors $\\\\mathbb{E}(X) = m$ et $V[X] = \\\\sigma^2$\\r\\n","\\r\\n","# **Connaissance transversales en probabilit\xe9s :**\\r\\n","## **1. Syst\xe8me d\'\xe9v\xe9nement complet :**\\r\\n","Soit $(\\\\Omega, A, P)$ un espace probabiliste. Un syst\xe8me complet d\'\xe9v\xe9nement est un syst\xe8mes o\xf9 tout les \xe9v\xe9nements r\xe9unis forment $\\\\Omega$. donc :\\r\\n","- $i \\\\neq j \\\\Longrightarrow A_i \\\\cup A_j = \\\\emptyset$;   (\xe9v\xe9nemenents ind\xe9pendants).\\r\\n","- $\\\\bigcup_{i \\\\in I} A_i = \\\\Omega$\\r\\n","\\r\\n","On parle aussi de syst\xe8me quasi-complet quand la deuxi\xe8me condition est remplac\xe9e par $\\\\sum_{i \\\\in I} P(A_i) = 1$.\\r\\n","\\r\\n","## **2. Exp\xe9rience ou sch\xe9ma de Bernoulli :**\\r\\n","Une exp\xe9rience de Bernoulli n\'a que deux issues possibles, succ\xe8s ($s$) ou \xe9chec ($\\\\overline{s}$).\\r\\n","Le succ\xe8s est d\xe9finit par une probabilit\xe9 $p$ et l\'\xe9chec par une probabilit\xe9 $q = 1 - p$.\\r\\n","Le param\xe8tre de l\'exp\xe9rience est $p$, la probabilit\xe9 d\'un succ\xe8s.  \\r\\n","**Exemple**: Pour un lancer de d\xe9, la probabilit\xe9 d\'avoir un 6, \xe9tant le succ\xe8s, est d\xe9finit par $p = \\\\frac{1}{6}$.  \\r\\n","$X$ est une variable al\xe9atoire qui est d\xe9finit par $X=1$ en cas de r\xe9ussite et $X=0$ en cas d\'\xe9chec. $X$ suit alors une loi de Bernoulli de param\xe8tre $p$ note $X \\\\sim \\\\mathcal{B}(p)$.  \\r\\n","$X$ admet une esp\xe9rence et une variance :\\r\\n","- $\\\\mathbb{E}[X] = 1 \\\\times p + 0 \\\\times (1 - p) = p$\\r\\n","- $V[X] = p(1-p)$\\r\\n","\\r\\n","## **3. Loi binomiale :**\\r\\n","On consid\xe8re une exp\xe9rience al\xe9atoire qui ne poss\xe8de que deux r\xe9sultats : succ\xe8s ($s$), \xe9chec ($\\\\overline{s}$). On pose :\\r\\n","- $p = P(s)$\\r\\n","- $q = P(\\\\overline{s}) = 1 - p$  \\r\\n","\\r\\n","On r\xe9p\xe8te $n$ fois cette exp\xe9rience, et on suppose que les r\xe9p\xe9titions sont ind\xe9pendentes. On pose $X$ une variable al\xe9atoire qui repr\xe9sente le nombre de succ\xe8s au cours des $n$ r\xe9p\xe9titions, on dit alors que $X$ suit une loi binomiale de param\xe8tre $n$ et $p$.  \\r\\n","La probabilit\xe9 d\'obtenir $k$ succ\xe8s au cours de $n$ r\xe9p\xe9titions est not\xe9 :\\r\\n","$$P(X = k) = C^n_k \\\\times p^k(1 - p)^{n-k}$$  \\r\\n","\\r\\n","O\xf9 $C^n_k$, dit, $k$ parmis $n$, est le nombre de combinaisons de $k$ \xe9lements choisis parmi $n$.\\r\\n","$C$ est le coefficient binomial et est \xe9gale au nombre de chemins conduisant \xe0 strictement $k$ succ\xe8s dans l\'arbre repr\xe9sentant le sch\xe9ma de Bernoulli. Plus formellement $C$ est d\xe9finit comme suit :\\r\\n","$$C_k^n = \\\\frac{n!}{k!(n - k)!}$$\\r\\n","\\r\\n","*Note* Une exp\xe9rience de Bernoulli est une loi Binomiale avec nombre de r\xe9p\xe9tition $n = 1$. Une loi Binomiale est la r\xe9p\xe9tition de $n$ fois le sch\xe9ma de Bernoulli.\\r\\n","\\r\\n","#### Quelques propri\xe9t\xe9 du coefficient Binomial\\r\\n","- $C_0^n = 1$, Nombre de chemin \xe0 0 succ\xe8s dans une suite de $n$ r\xe9p\xe9titions ? un seul chemin possible.\\r\\n","- De la m\xeame mani\xe8re: $C_n^n = 1$.\\r\\n","- $C_1^n = n$, un seul succ\xe8s sur $n$ r\xe9p\xe9titions: $n$ combinaisons possibles; $n=4 \\\\Longrightarrow (1000, 0100, 0010, 0001)$.\\r\\n","- Pour $0 \\\\leq k \\\\leq n$: $C^n_{n-k} = C^n_k$.\\r\\n","- Pour $0 \\\\leq k \\\\leq n$: $C^n_k + C^n_{k+1} = C^{n+1}_{k+1}$\\r\\n","- En utilisant les r\xe9gles pr\xe9c\xe9dentes, on peut simplifier le calcul. **Exemple**\\r\\n","    - $C^4_2 = C^{3 + 1}_{1 + 1} = C^3_1 + C^3_2 = 3 + C^2_1 + C^2_2 = 3 + 2 +1 = 6$\\r\\n","\\r\\n","## **4. Loi de Bayes :**\\r\\n","Pour bien comprendre la loi de Bayes, il faut comprendre la notion de d\xe9pendance entre \xe9v\xe9nements. Soit $A$ et $B$ deux \xe9v\xe9nements :\\r\\n","- Si $A$ et $B$ sont ind\xe9pendants, alors : \\r\\n","    - $P(A \\\\cap B) = P(A)P(B) = P(B)P(A) = P(B \\\\cap A)$  \\r\\n","    - $P(A | B) = \\\\frac{P(A \\\\cap B)}{P(B)} = \\\\frac{P(A)P(B)}{P(B)} = P(A)$\\r\\n","- Si $A$ et $B$ sont d\xe9pendants, alors : \\r\\n","    - $P(A \\\\cap B) = P(B)P(A|B) = P(A)P(B|A) = P(B \\\\cap A)$  \\r\\n","\\r\\n","$P(A \\\\cap B)$ est la probabilit\xe9 de l\'arriv\xe9e des \xe9v\xe9nements $A$ et $B$. Dans le cas d\'une d\xe9pendances entre ces d\xe9rniers, il faut alors prendre en consid\xe9ration l\'ordre d\'arriv\xe9. Si $A$ est arrive en premier, la probabilit\xe9 d\'avoir $B$ lorsque $A$ est pr\xe9sent est d\xe9finie par $P(B|A)$ ($B$ sachant $A$, et aussi not\xe9 $P_A(B)$). On aura donc $P(A \\\\cap B) = P(A) \\\\times P(B|A)$. Dans un deuxi\xe8me cas o\xf9 $B$ arrive en premier, on aura alors $P(B) \\\\times P(A|B)$.  \\r\\n","Par inf\xe9rence, on a alors :\\r\\n","$$A \\\\cap B = B \\\\cap A \\\\Longleftrightarrow P(A \\\\cap B) = P(B \\\\cap A) \\\\Longleftrightarrow P(B)P(A|B) = P(A)P(B|A)$$  \\r\\n","Ce qui nous donne la loi de Bayes d\xe9finit par :\\r\\n","$$ P(A|B) = \\\\frac{P(A)P(B|A)}{P(B)}$$  \\r\\n","Il est \xe0 noter aussi que dans un syst\xe8me d\'\xe9v\xe9nements complet $(\\\\Omega, A, P)$ o\xf9 toutes les probabilit\xe9s $P_i$ des \xe9v\xe9nements $A_i$ sont non nulles. La loi de Bayes est d\xe9finit par :\\r\\n","$$ B = B \\\\cap A_1 + B \\\\cap A_2 + ... + B \\\\cap A_n = \\\\sum_{i \\\\in A} B \\\\cap A_i$$\\r\\n","$$ P(B) = \\\\sum_{n \\\\geq 1} P(A_n) \\\\times P(B | A_n)$$  \\r\\n","Formule souvent utilis\xe9e quand le syst\xe8me est constitu\xe9 de $A$ et $\\\\overline{A}$ :\\r\\n","$$ P(A|B) = \\\\frac{P(B|A)P(A)}{P(B|A)P(A) + P(B|\\\\overline{A})P(\\\\overline{A})} $$  \\r\\n","*Note* Attention, on se permet d\'utiliser $\\\\overline{A}$ car on peut la d\xe9duire \xe9tant donn\xe9 que le syst\xe8me n\'est constitu\xe9 que des deux \xe9v\xe9nements contraires. Dans un syst\xe8mes avec plus d\'\xe9v\xe9nements, il faudra avoir plus d\'informations sur les autres \xe9v\xe9nements. De mani\xe8re plus g\xe9n\xe9rale on notera :\\r\\n","$$ P(A_k|B) = \\\\frac{P(B|A_k)P(A_k)}{P(B)} = \\\\frac{P(B|A_k)P(A_k)}{\\\\sum_{i \\\\geq 1}^n P(A_i)P(B |A_i)} \\\\;\\\\; avec \\\\;\\\\; k \\\\neq i$$  \\r\\n","\\r\\n","**Exemple pour avoir une meilleure id\xe9e**: Posons deux \xe9v\xe9nements $A$ = \\"tomber malade\\" et $B$ = \\"\xeatre test\xe9 positif\\". La probabilit\xe9 de tomber malade ($A$ arrive en premier) puis \xeatre test\xe9 positif ($B$ arrive en second) est assez \xe9l\xe9v\xe9e, soit par exemple $90\\\\%$, si on suppose que le test est de bonne qualit\xe9. Cependant, la probabilit\xe9 d\'\xeatre test\xe9 positif ($B$ arrive en premier) puis de tomber malade ($A$ arrive en second) est assez basse, il est moins probable de tomber malade en ayant d\xe9j\xe0 \xe9t\xe9 test\xe9 positif.  \\r\\n","Soit $P(A) = 30\\\\%$ et $P(B|A) = 90\\\\%$, en utilisant les formules pr\xe9c\xe9dentes, il est possible de calculer $P(A|B)$:\\r\\n","- $P(B) = P(A)P(B|A) + P(\\\\overline{A})P(B|\\\\overline{A}) = 0,3 \\\\times 0,9 + 0,7 \\\\times 0,1 = 0,34$\\r\\n","- $P(A|B) = \\\\frac{P(A)P(B|A)}{P(B)} = \\\\frac{0,3 \\\\times 0,9}{0,34} = 0,79$\\r\\n","\\r\\n","# **Processus stochastique**\\r\\n","Un processus stochastique est une fonction du temps dont la valeur \xe0 chaque instant $t$ d\xe9pend de l\'issue d\'une exp\xe9rience al\xe9atoire.\\r\\n","- Le temps peut \xeatre discret ou continu.\\r\\n","- L\'ensemble $E$ est l\'ensemble des \xe9tats possibles que peut prendre la variable $X^{(t)}$.  \\r\\n","## Vecteur et matrice stochastique :\\r\\n","- Un vecteur $\\\\pi = (\\\\pi_0, \\\\pi_1, \\\\pi_2, ..., \\\\pi_n)$ est stochastique seulement si la somme des composantes est \xe9gale \xe0 1 : $\\\\sum_i^n \\\\pi_i = 1$.\\r\\n","- Pareil pour une matrice, chaque ligne est un vecteur stochastique.  \\r\\n","**Exemple**\\r\\n","$$ \\\\pi = (\\\\frac{1}{4}, \\\\frac{1}{2}, \\\\frac{1}{4}); \\\\;\\\\;\\\\; P = \\r\\n","\\\\begin{bmatrix}\\r\\n","    \\\\frac{1}{2} & \\\\frac{1}{2} & \\\\frac{1}{4} & = 1\\\\\\\\ \\r\\n","    \\\\frac{1}{3} & \\\\frac{1}{3} & \\\\frac{1}{3} & = 1\\\\\\\\\\r\\n","    \\\\frac{1}{2} & 0 & \\\\frac{1}{2} & = 1\\\\\\\\ \\r\\n","\\\\end{bmatrix}$$\\r\\n","\\r\\n","## Processus sans m\xe9moire (Processus Markovien) :\\r\\n","Un processus Markovien est un processus ayant la propri\xe9t\xe9 de Markov. L\'\xe9volution du processus ne d\xe9pend que du pr\xe9sent, c-\xe0-dire qu\'\xe0 un instant $t+1$, on ne d\xe9pend que de l\'\xe9tat \xe0 l\'instant $t$ :\\r\\n","$$ P[X_{n+1} = x_{n+1} | X_0=x_0; X_1=x_1; ...; X_n=x_n] = P[X_{n+1} = x_{x+1}|X_n=x_n]$$ \\r\\n","\\r\\n","# **Les chaines de Markov**\\r\\n","## **1. Les chaines de Markov \xe0 temps discret :**\\r\\n","$E$ est un espace d\'\xe9tats discrets, peut \xeatre fini ou d\xe9nombrable. ${X_n}_{n \\\\in E}$ est une variable al\xe9atoire qui mod\xe9lise une chaine de Markov \xe0 temps discret (CMTD) seulement si :\\r\\n","$$ P[X_n = j | X_{n - 1} = i_{n - 1}; X_{n - 2} = i_{n-2}; ...; X_0 = i_0] = P[X_j = j|X_{n - 1} = i_{n-1}]$$  \\r\\n","$X$ repr\xe9sente un processus sans m\xe9moire.  \\r\\n","### **Propri\xe9t\xe9s d\'une CMTD :**\\r\\n","- On pose $P_{ij}$ la probabilit\xe9 de transiter vers l\'\xe9tat $j$ sachant qu\'\xe0 l\'instant $t-1$ on etait \xe0 l\'\xe9tat $i$.\\r\\n","$$ P_{ij} = P[X_n = j | X_{n-1}=i]; \\\\;\\\\; \\\\forall n \\\\in \\\\mathbb{N}$$\\r\\n","- On note aussi la somme des probabilit\xe9s d\'une transition d\'un \xe9tat $i$ vers $j$ est \xe9gale \xe0 1: $\\\\sum_{j \\\\in E} P_{ij} = 1$.\\r\\n","- Il est possible de transiter vers le m\xeame \xe9tat : $P_{ii} \\\\geq 0$.  \\r\\n","**Exemple**  \\r\\n","$E = \\\\{1, 2, 3, 4\\\\}$; $P = [P_{ij}]; i,j \\\\in E$\\r\\n","$$ P = \\\\begin{bmatrix}\\r\\n","    0 & P_{12} & 0 & P_{14} & = 1 \\\\\\\\\\r\\n","    0 & 0 & 1 & 0 & =1 \\\\\\\\\\r\\n","    P_{31} & 0 & P_{33} & 0 & =1 \\\\\\\\\\r\\n","    1 & 0 & 0 & 0 & = 1 \\r\\n","\\\\end{bmatrix}$$  \\r\\n","*Note* Peut \xeatre repr\xe9sent\xe9 sous la forme d\'un graphe pond\xe9r\xe9 o\xf9 les poids sont les probabilit\xe9s de transition.\\r\\n","\\r\\n","### **Distribution de l\'\xe9tat initial** :\\r\\n","- L\'\xe9tat initial est d\xe9fini par le vecteur $\\\\pi^{(0)} = (\\\\pi^{(0)}_0, \\\\pi^{(0)}_1, \\\\pi^{(0)}_2, ..., \\\\pi^{(0)}_n)$ o\xf9 $\\\\pi^{(0)}_i = P[X_0 = i]$ (Probabilit\xe9 que la chaine se trouve \xe0 l\'\xe9tat $i$ \xe0 l\'instant $0$).  \\r\\n","- Si un syst\xe8me est initialement \xe0 l\'\xe9tat $j$ alors $\\\\pi^{(0)}_j = 1$ et $\\\\pi^{(0)}_i = 0$; $\\\\forall i \\\\neq j$.\\r\\n","\\r\\n","## **2. Analyse d\'une chaine de Markov \xe0 temps discret** :\\r\\n","### **R\xe9gime transitoire :**\\r\\n","Afin de d\xe9terminer le vecteur $\\\\pi^{(n)}$ des probabilit\xe9s d\'\xe9tats \xe0 un instant $n$, on pose :\\r\\n","- \xc9tat du vecteur $n$ : $\\\\pi^{(n)} = [\\\\pi^{(n)}_j]_{j \\\\in E} = [\\\\pi^{(n)}_1, \\\\pi^{(n)}_2, \\\\pi^{(n)}_3, ..., \\\\pi^{(n)}_j]$.\\r\\n","- Les valeurs de transition d\xe9pendent de la matrice de transition $P$ :\\r\\n","$$\\\\pi^{(n)}_j = P[X_n = j] = \\\\sum_{i \\\\in E} P[X_{n - 1} = i] \\\\times P[X_n = j | X_{n - 1} = i] \\\\;\\\\; Loi \\\\;\\\\; de \\\\;\\\\; Bayes$$\\r\\n","$$\\\\pi^{(n)}_j = \\\\sum_{i \\\\in E} \\\\pi^{(n-1)}_j \\\\times P_{ij}$$  \\r\\n","Par r\xe9cursion jusqu\'\xe0 $0$, on obtient :\\r\\n","$$\\\\pi^{(n)}_j = \\\\pi^{(n-1)} \\\\times P = \\\\pi^{(0)}P^n$$  \\r\\n","**Explication**  \\r\\n","$P[X_n = j]$ est la probabilit\xe9 d\'\xeatre \xe0 l\'\xe9tat $n$ \xe0 l\'instant $j$ et est not\xe9 $\\\\pi{(n)}_j$. Pour obtenir cette probabilit\xe9 d\'\xeatre \xe0 l\'\xe9tat $n$, il faut sommer toutes les probabilit\xe9s de transition vers cette \xe9tat en d\xe9marrant de l\'\xe9tat pr\xe9c\xe9dent $i$ (Il faut consid\xe9rer tout les chemins possibles). Plus formellement cela donne $P[X_{n - 1} = i] \\\\times P[X_n = j | X_{n - 1} = i]$ pour un seul chemin, en sommant le tout on obtient $\\\\Longleftrightarrow \\\\sum_{i \\\\in E} P[X_{n - 1} = i] \\\\times P[X_n = j | X_{n - 1} = i]$.  \\r\\n","Pour encore mieux comprendre, il faut savoir que $P[X_n = i]$ est simplement la probabilit\xe9 d\'\xeatre \xe0 l\'\xe9tat $i$ qu\'on note $\\\\pi$. quant \xe0 $P[X_n = i| X_{n - 1} = j]$, c\'est la probabilit\xe9 de **transiter** de l\'\xe9tat $j$ vers $i$ et qu\'on note $P$ (C\'est une matrice).  \\r\\n","Quant \xe0 la r\xe9cursion jusu\'\xe0 $0$, voici un d\xe9roulement pour mieux comprendre :\\r\\n","- $\\\\pi^{(1)} = \\\\pi^{(0)} \\\\times P$\\r\\n","- $\\\\pi^{(2)} = \\\\pi^{(1)} \\\\times P = \\\\pi^{(0)} \\\\times P^2$\\r\\n","- $\\\\pi^{(3)} = \\\\pi^{(2)} \\\\times P = \\\\pi^{(0)} \\\\times P^3$\\r\\n","- ...\\r\\n","- $\\\\pi^{(n)} = \\\\pi^{(0)} \\\\times P^n$ *Appell\xe9 formule de probabilit\xe9s totales*\\r\\n","\\r\\n","### **\xc9volution globale du processus $X_n$ ($m$ \xe9tapes) :** ... \xe0 suivre"],"metadata":{}},{"cell_type":"markdown","source":[],"metadata":{}}],"metadata":{"orig_nbformat":4,"language_info":{"name":"python","version":"3.6.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3.6.12 64-bit (\'deeplearning\': conda)"},"interpreter":{"hash":"101b61497117ea3f18d4e0f8cf93eb2d64c16663f47aa00fa1289b89b66d7e41"}},"nbformat":4,"nbformat_minor":2}')}},function(e){e.O(0,[774,523,112,594,871,888,179],(function(){return n=55809,e(e.s=n);var n}));var n=e.O();_N_E=n}]);